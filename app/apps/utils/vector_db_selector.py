"""
ç®€å•çš„å‘é‡æ•°æ®åº“é€‰æ‹©å™¨

æ”¯æŒChromaDBå’ŒQdrantä¸¤ä¸ªæ•°æ®åº“çš„åˆ‡æ¢
"""
import traceback
from typing import List, Dict, Any

import torch
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_qdrant import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchValue
from qdrant_client.http.models import VectorParams
from qdrant_client.models import Distance

from apps.models.document import Document as DocumentModel, DocumentChunk
from apps.utils.common import get_local_model_path
from config import CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION, SIMILARITY_THRESHOLD
from config import (
    EMBEDDING_MODEL, HF_HOME, HF_OFFLINE,
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION_NAME
)
from config import VECTOR_DB_TYPE


class VectorDBSelector:
    """å‘é‡æ•°æ®åº“é€‰æ‹©å™¨"""

    def __init__(self):
        self.db_type = VECTOR_DB_TYPE
        self.vectorstore = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.embeddings = self._get_embedding_model()
        self._init_database()

    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        if self.db_type == "qdrant":
            self._init_qdrant()
        else:
            self._init_chroma()

    def _get_embedding_model(self):
        """è·å–åµŒå…¥æ¨¡å‹"""
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        local_model_path = get_local_model_path(EMBEDDING_MODEL, HF_HOME)

        if local_model_path and HF_OFFLINE:
            embeddings = HuggingFaceEmbeddings(
                model_name=local_model_path,
                model_kwargs={'device': self.device},
                encode_kwargs={'normalize_embeddings': True}
            )
        else:
            embeddings = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL,
                cache_folder=HF_HOME,
                model_kwargs={'device': self.device},
                encode_kwargs={'normalize_embeddings': True}
            )
        return embeddings

    def _init_chroma(self):
        """åˆå§‹åŒ–ChromaDB"""
        self.vectorstore = Chroma(
            collection_name=CHROMA_COLLECTION,
            embedding_function=self.embeddings,
            persist_directory=CHROMA_PERSIST_DIRECTORY,
        )
        print("âœ… ä½¿ç”¨ChromaDBå‘é‡å­˜å‚¨")

    def _init_qdrant(self):
        """åˆå§‹åŒ– Qdrantï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        host, port, collection_name = QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION_NAME

        # âœ… å¿…é¡»ç”¨ url= æ˜¾å¼æŒ‡å®š HTTP è®¿é—®ï¼Œå¦åˆ™é»˜è®¤èµ° gRPCï¼
        client = QdrantClient(url=f"http://{host}:{port}", timeout=30)
        print(f"ğŸŒ æ­£åœ¨è¿æ¥ Qdrant: http://{host}:{port}")

        # âœ… å¦‚æœ collection ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
        if not client.collection_exists(collection_name):
            dim = self.embeddings.client.get_sentence_embedding_dimension()
            client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
            )
            print(f"âœ… è‡ªåŠ¨åˆ›å»º Qdrant collection: {collection_name} (dim={dim})")

        # âœ… åˆå§‹åŒ– LangChain å‘é‡å­˜å‚¨
        self.vectorstore = Qdrant(
            client=client,
            collection_name=collection_name,
            embeddings=self.embeddings,
        )
        print("âœ… ä½¿ç”¨Qdrantå‘é‡å­˜å‚¨")


    async def add_documents_from_chunks(self, document_id: int, chunks: List[str], chunk_objects: List,
                                        metadata: Dict[str, Any] = None) -> List[str]:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        try:
            if not chunks or not chunk_objects:
                raise Exception("æ–‡æ¡£å—ä¸ºç©º")

            # åˆ›å»ºLangChainæ–‡æ¡£å¯¹è±¡
            documents = []
            chunk_ids = []

            for i, (chunk_text, chunk_obj) in enumerate(zip(chunks, chunk_objects)):
                doc_metadata = {
                    "document_id": document_id,
                    "chunk_id": chunk_obj.id,
                    "chunk_index": i,
                    "source": metadata.get("filename",
                                           f"document_{document_id}") if metadata else f"document_{document_id}",
                }

                langchain_doc = Document(
                    page_content=chunk_text,
                    metadata=doc_metadata
                )

                documents.append(langchain_doc)
                chunk_ids.append(str(chunk_obj.id))

            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            if documents:
                self.vectorstore.add_documents(documents)

            return chunk_ids

        except Exception as e:
            traceback.print_exc()
            raise Exception(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨å¤±è´¥: {e}")

    async def search_similar_documents(self, query: str, top_k: int = 5, use_threshold: bool = True) -> List[
        Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        try:
            # æ‰§è¡Œæœç´¢
            print(f"ğŸ” æ‰§è¡Œå‘é‡æœç´¢: æŸ¥è¯¢='{query}', top_k={top_k}, æ•°æ®åº“ç±»å‹={self.db_type}")
            results = self.vectorstore.similarity_search_with_score(query, k=20)
            print(f"ğŸ“Š æœç´¢ç»“æœæ•°é‡: {len(results)}")

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            all_results = []
            filtered_results = []

            for i, (doc, score) in enumerate(results):
                # è®¡ç®—ç›¸ä¼¼åº¦
                if self.db_type == "qdrant":
                    similarity = score
                else:
                    similarity = max(0.0, 1.0 - score)
                print(f"ğŸ“ˆ ç»“æœ {i + 1}: score={score:.4f}, similarity={similarity:.4f}")
                # è·å–å…ƒæ•°æ®
                metadata = doc.metadata
                print(f"ğŸ“„ æ–‡æ¡£å…ƒæ•°æ®: {metadata}")
                document_id = metadata.get('document_id')
                chunk_id = metadata.get('chunk_id')

                if document_id and chunk_id:
                    try:
                        # è·å–æ•°æ®åº“ä¸­çš„æ–‡æ¡£å’Œå—ä¿¡æ¯
                        document = await DocumentModel.get_or_none(id=document_id)
                        if not document:
                            await vector_search.delete_document(document_id)
                            continue
                        chunk = await DocumentChunk.get_or_none(id=chunk_id)
                        if not chunk:
                            await vector_search.delete_document(document_id)
                            continue

                        if document and chunk:
                            result_item = {
                                'document': document,
                                'chunk': chunk,
                                'similarity': similarity,
                                'metadata': metadata,
                                'above_threshold': similarity >= SIMILARITY_THRESHOLD
                            }

                            all_results.append(result_item)

                            # å¦‚æœä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåªä¿ç•™ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ç»“æœ
                            if use_threshold and similarity >= SIMILARITY_THRESHOLD:
                                filtered_results.append(result_item)
                    except Exception as e:
                        print(f"å¤„ç†æœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                        continue

            # é€‰æ‹©è¿”å›ç»“æœ
            print(f"ğŸ“‹ è¿‡æ»¤å‰ç»“æœ: {len(all_results)}, è¿‡æ»¤åç»“æœ: {len(filtered_results)}")

            if filtered_results:
                filtered_results.sort(key=lambda x: x['similarity'], reverse=True)
                print(f"âœ… è¿”å›è¿‡æ»¤åçš„ç»“æœ: {len(filtered_results)}")
                return filtered_results[:top_k]
            elif all_results:
                all_results.sort(key=lambda x: x['similarity'], reverse=True)
                print(f"âš ï¸ è¿”å›æ‰€æœ‰ç»“æœ: {len(all_results)}")
                return all_results[:min(top_k, 3)]
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœ")
                return []

        except Exception as e:
            traceback.print_exc()
            print(f"æœç´¢ç›¸ä¼¼æ–‡æ¡£å¤±è´¥: {e}")
            return []

    async def delete_document(self, document_id: int):
        """åˆ é™¤æ–‡æ¡£"""
        try:
            if self.db_type == "qdrant":
                # Qdrantåˆ é™¤
                self.vectorstore.client.delete(
                    collection_name=QDRANT_COLLECTION_NAME,
                    points_selector=Filter(
                        must=[
                            FieldCondition(
                                key="document_id",
                                match=MatchValue(value=document_id)
                            )
                        ]
                    )
                )
            else:
                # ChromaDBåˆ é™¤
                self.vectorstore._collection.delete(where={"document_id": document_id})

            print(f"âœ… æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id} çš„å‘é‡æ•°æ®")

        except Exception as e:
            traceback.print_exc()
            raise Exception(f"åˆ é™¤æ–‡æ¡£ {document_id} å‘é‡æ•°æ®å¤±è´¥: {e}")

    async def count_vectors(self) -> int:
        """ç»Ÿè®¡å‘é‡æ•°é‡"""
        try:
            if self.db_type == "qdrant":
                # Qdrantç»Ÿè®¡
                collection_info = self.vectorstore.client.get_collection(QDRANT_COLLECTION_NAME)
                return collection_info.points_count
            else:
                # ChromaDBç»Ÿè®¡
                collection = self.vectorstore._collection
                return collection.count()
        except Exception as e:
            print(f"ç»Ÿè®¡å‘é‡æ•°é‡å¤±è´¥: {e}")
            return 0


vector_search = VectorDBSelector()
print("âœ… ä½¿ç”¨LangChainå‘é‡å­˜å‚¨")
