"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—

è¯¥æ¨¡å—åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
1. DocumentProcessor: æ–‡æ¡£å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–
2. VectorSearch: å‘é‡æœç´¢å¼•æ“ï¼ŒåŸºäºChromaæ•°æ®åº“è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
3. RAGGenerator: RAGç”Ÿæˆå™¨ï¼Œé›†æˆLangChainå’ŒOpenAIè¿›è¡Œæ™ºèƒ½é—®ç­”

æŠ€æœ¯æ ˆï¼š
- æ–‡æ¡£è§£æ: ç‹¬ç«‹çš„DocumentParseræ¨¡å—
- æ–‡æœ¬åˆ†å‰²: LangChain RecursiveCharacterTextSplitter
- å‘é‡åŒ–: Sentence Transformers
- å‘é‡å­˜å‚¨: ChromaDB
- æ™ºèƒ½é—®ç­”: LangChain + OpenAI GPT

æ¶æ„è¯´æ˜ï¼š
- æ–‡æ¡£è§£æåŠŸèƒ½å·²æ‹†åˆ†åˆ°ç‹¬ç«‹çš„document_parseræ¨¡å—
- æœ¬æ¨¡å—ä¸“æ³¨äºå‘é‡åŒ–ã€æœç´¢å’Œç”ŸæˆåŠŸèƒ½
- é€šè¿‡ä¾èµ–æ³¨å…¥çš„æ–¹å¼ä½¿ç”¨æ–‡æ¡£è§£æå™¨
- ç°åœ¨æ”¯æŒLangChainé›†æˆï¼Œæä¾›æ›´å¼ºå¤§çš„æ–‡æ¡£å¤„ç†èƒ½åŠ›
"""

import traceback
from typing import List, Dict, Any

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from apps.utils.vector_db_selector import vector_search
from config import (
    OPENAI_API_KEY, OPENAI_BASE_URL
)


class RAGGenerator:
    """
    RAGç”Ÿæˆå™¨ - æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. é›†æˆå‘é‡æœç´¢å’ŒLLMç”Ÿæˆ
    2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
    3. æ”¯æŒå¤šç§å›ç­”æ¨¡å¼ï¼ˆLLMæ™ºèƒ½å›ç­” / ç®€å•å›ç­”ï¼‰
    4. æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—®ç­”ä½“éªŒ
    """

    def __init__(self):
        """
        åˆå§‹åŒ–RAGç”Ÿæˆå™¨
        
        ç»„ä»¶ï¼š
        1. LangChainå‘é‡å­˜å‚¨: å‘é‡æœç´¢å¼•æ“
        2. LangChain + OpenAI: æ™ºèƒ½é—®ç­”é“¾ï¼ˆå¯é€‰ï¼‰
        3. å¤‡ç”¨ç®€å•å›ç­”æ¨¡å¼
        """
        try:
            # ä½¿ç”¨LangChainå‘é‡å­˜å‚¨
            self.vector_search = vector_search

            # åˆå§‹åŒ–LLMç»„ä»¶
            self.llm = None
            self.chain = None
            self.llm_available = False

            # æ£€æŸ¥OpenAIé…ç½®å¹¶åˆå§‹åŒ–LLM
            if OPENAI_API_KEY and OPENAI_API_KEY.strip():
                try:
                    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
                    self.llm = ChatOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿å›ç­”çš„ä¸€è‡´æ€§
                        model="gpt-3.5-turbo",
                        max_tokens=2000  # é™åˆ¶å›ç­”é•¿åº¦
                    )

                    # åˆ›å»ºç³»ç»Ÿæç¤ºæ¨¡æ¿
                    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å’Œè®¾å¤‡ä¿¡æ¯é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å’Œè®¾å¤‡ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å’Œè®¾å¤‡ä¿¡æ¯å›ç­”ï¼Œä¸è¦æ·»åŠ è¿™äº›ä¿¡æ¯ä¸­æ²¡æœ‰çš„å†…å®¹
2. å¦‚æœæ–‡æ¡£å’Œè®¾å¤‡ä¿¡æ¯ä¸­éƒ½æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³å†…å®¹"
3. å¦‚æœæä¾›äº†è®¾å¤‡ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨è®¾å¤‡ä¿¡æ¯å›ç­”å…³äºè®¾å¤‡çŠ¶æ€ã€ä½ç½®ã€å®‰è£…ç­‰çš„é—®é¢˜
4. å¯¹äºè®¾å¤‡ç»Ÿè®¡ç±»é—®é¢˜ï¼Œè¯·åŸºäºæä¾›çš„è®¾å¤‡ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œå›ç­”ï¼š
   - å¦‚æœè¯¢é—®æ€»è®¾å¤‡æ•°ï¼Œç›´æ¥å›ç­”ç»Ÿè®¡ä¿¡æ¯ä¸­çš„æ€»è®¾å¤‡æ•°
   - å¦‚æœè¯¢é—®ç‰¹å®šçŠ¶æ€è®¾å¤‡æ•°ï¼Œä½†ç»Ÿè®¡ä¿¡æ¯ä¸­æ²¡æœ‰è¯¥çŠ¶æ€çš„å…·ä½“æ•°æ®ï¼Œè¯·è¯´æ˜"æ ¹æ®æä¾›çš„è®¾å¤‡ç»Ÿè®¡ä¿¡æ¯ï¼Œæ— æ³•ç¡®å®šç‰¹å®šçŠ¶æ€çš„è®¾å¤‡æ•°é‡"
   - å¦‚æœç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤ºæ€»è®¾å¤‡æ•°ä¸º0ï¼Œè¯·è¯´æ˜"æ ¹æ®æä¾›çš„è®¾å¤‡ç»Ÿè®¡ä¿¡æ¯ï¼Œå½“å‰æ²¡æœ‰è®¾å¤‡"
5. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„ï¼š
   - ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜ç»„ç»‡å†…å®¹
   - é‡è¦ä¿¡æ¯ç”¨**ç²—ä½“**æ ‡è®°
   - ä½¿ç”¨é¡¹ç›®ç¬¦å·(â€¢)æˆ–æ•°å­—åˆ—è¡¨å±•ç¤ºè¦ç‚¹
   - æ¯ä¸ªæ®µè½ä¸“æ³¨ä¸€ä¸ªä¸»é¢˜ï¼Œæ®µè½é—´ç•™ç©ºè¡Œ
   - å¤æ‚å†…å®¹ä½¿ç”¨è¡¨æ ¼æˆ–ç»“æ„åŒ–æ ¼å¼
6. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œè®¾å¤‡ä¿¡æ¯ï¼Œæ ¼å¼ï¼šã€Œæ–‡æ¡£åï¼šå…·ä½“å†…å®¹ã€æˆ–ã€Œè®¾å¤‡åï¼šå…·ä½“ä¿¡æ¯ã€
7. å¦‚æœæœ‰å¤šä¸ªä¿¡æ¯æ¥æºæä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶æ ‡æ˜ä¿¡æ¯æ¥æº
8. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚å½“ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{document_context}

æä¾›çš„è®¾å¤‡ä¿¡æ¯ï¼š
{device_context}"""

                    human_template = "é—®é¢˜ï¼š{question}"

                    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
                    chat_prompt = ChatPromptTemplate([
                        ("system", system_template),
                        ("human", human_template),
                    ])

                    # åˆ›å»ºè¾“å‡ºè§£æå™¨
                    output_parser = StrOutputParser()

                    # åˆ›å»ºLangChainå¤„ç†é“¾
                    self.chain = chat_prompt | self.llm | output_parser
                    self.llm_available = True

                except Exception:
                    self.llm = None
                    self.chain = None
                    self.llm_available = False
            else:
                self.llm_available = False

        except Exception as e:
            raise Exception(f"RAGGeneratoråˆå§‹åŒ–å¤±è´¥: {e}")

    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]], device_context: str = "") -> str:
        """
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        å›ç­”ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨LLMæ™ºèƒ½å›ç­”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. é™çº§åˆ°ç®€å•å›ç­”æ¨¡å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        3. æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ¥æºå¼•ç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å—
            device_context: è®¾å¤‡ä¿¡æ¯ä¸Šä¸‹æ–‡
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£æˆ–è®¾å¤‡ä¿¡æ¯
            if not context_chunks and not device_context:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æˆ–è®¾å¤‡ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£æˆ–æ·»åŠ è®¾å¤‡ä¿¡æ¯ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"

            # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            print(f"RAGç”Ÿæˆå™¨æ¥æ”¶åˆ°æ–‡æ¡£æ•°é‡: {len(context_chunks)}")
            
            for i, chunk in enumerate(context_chunks, 1):
                # å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼
                if chunk.get('document') and chunk.get('chunk'):
                    doc_name = chunk['document'].original_filename or chunk['document'].filename
                    content = chunk['chunk'].content
                    print(f"æ–‡æ¡£ {i}: {doc_name}, å†…å®¹é•¿åº¦: {len(content) if content else 0}")
                else:
                    # ä½¿ç”¨å¤‡ç”¨æ ¼å¼
                    doc_name = chunk.get('metadata', {}).get('source', 'æœªçŸ¥æ–‡æ¡£')
                    content = chunk.get('content', 'æ— å†…å®¹')
                    print(f"å¤‡ç”¨æ ¼å¼æ–‡æ¡£ {i}: {doc_name}, å†…å®¹é•¿åº¦: {len(content) if content else 0}")

                similarity = chunk.get('similarity', 0)

                # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
                context_part = f"""æ–‡æ¡£ {i}: {doc_name}
ç›¸ä¼¼åº¦: {similarity:.2f}
å†…å®¹: {content}"""
                context_parts.append(context_part)

            document_context = "\n\n" + "=" * 50 + "\n\n".join(context_parts) if context_parts else "æ— ç›¸å…³æ–‡æ¡£"
            print(f"æ„å»ºçš„æ–‡æ¡£ä¸Šä¸‹æ–‡é•¿åº¦: {len(document_context)}")

            # å¦‚æœæ²¡æœ‰è®¾å¤‡ä¿¡æ¯ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
            if not device_context:
                device_context = "æ— ç›¸å…³è®¾å¤‡ä¿¡æ¯"

            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    answer = await self._llm_answer(query, document_context, device_context)

                    # ä¸å†è‡ªåŠ¨æ·»åŠ æ¥æºä¿¡æ¯ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦æ˜¾ç¤º
                    return answer

                except Exception:
                    return self._simple_answer(query, context_chunks, device_context)
            else:
                return self._simple_answer(query, context_chunks, device_context)

        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def generate_answer_stream(self, query: str, context_chunks: List[Dict[str, Any]], device_context: str = ""):
        """
        æµå¼ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ–‡æ¡£ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            device_context: è®¾å¤‡ä¿¡æ¯ä¸Šä¸‹æ–‡
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
            context_parts = []
            print(f"æµå¼RAGç”Ÿæˆå™¨æ¥æ”¶åˆ°æ–‡æ¡£æ•°é‡: {len(context_chunks)}")
            
            for i, chunk in enumerate(context_chunks, 1):
                # ç›´æ¥ä½¿ç”¨æ•°æ®ç»“æ„
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                content = chunk['chunk'].content
                print(f"æµå¼æ–‡æ¡£ {i}: {doc_name}, å†…å®¹é•¿åº¦: {len(content) if content else 0}")
                context_parts.append(f"æ–‡æ¡£{i}: {doc_name}\nå†…å®¹: {content}")

            document_context = "\n\n" + "=" * 50 + "\n\n".join(context_parts) if context_parts else "æ— ç›¸å…³æ–‡æ¡£"
            print(f"æµå¼æ„å»ºçš„æ–‡æ¡£ä¸Šä¸‹æ–‡é•¿åº¦: {len(document_context)}")
            
            # å¦‚æœæ²¡æœ‰è®¾å¤‡ä¿¡æ¯ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
            if not device_context:
                device_context = "æ— ç›¸å…³è®¾å¤‡ä¿¡æ¯"

            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    print(f"ä½¿ç”¨LLMæµå¼ç”Ÿæˆå›ç­”")
                    async for chunk in self._llm_answer_stream(query, document_context, device_context):
                        yield chunk
                except Exception as e:
                    print(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {e}")
                    print(f"é™çº§åˆ°ç®€å•å›ç­”æ¨¡å¼")
                    # é™çº§åˆ°ç®€å•å›ç­”
                    simple_answer = self._simple_answer(query, context_chunks, device_context)
                    yield simple_answer
            else:
                print(f"LLMä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼")
                # éLLMæ¨¡å¼ï¼Œç›´æ¥è¿”å›ç®€å•å›ç­”
                simple_answer = self._simple_answer(query, context_chunks, device_context)
                yield simple_answer

        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def _llm_answer_stream(self, query: str, document_context: str, device_context: str):
        """
        ä½¿ç”¨LangChainé“¾å¼æµå¼ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            document_context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            device_context: è®¾å¤‡ä¿¡æ¯ä¸Šä¸‹æ–‡
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„chainè¿›è¡Œæµå¼è°ƒç”¨
            if self.chain:
                async for chunk in self.chain.astream({
                    "question": query,
                    "document_context": document_context,
                    "device_context": device_context
                }):
                    if chunk:
                        yield chunk
            else:
                raise Exception("LangChainå¤„ç†é“¾æœªåˆå§‹åŒ–")

        except Exception as e:
            traceback.print_exc()
            raise Exception(f"LLMé“¾å¼æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")

    async def _llm_answer(self, query: str, document_context: str, device_context: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            document_context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            device_context: è®¾å¤‡ä¿¡æ¯ä¸Šä¸‹æ–‡
            
        Returns:
            str: LLMç”Ÿæˆçš„å›ç­”
        """
        try:
            # è°ƒç”¨LangChainå¤„ç†é“¾
            response = await self.chain.ainvoke({
                "question": query,
                "document_context": document_context,
                "device_context": device_context
            })

            if not response or not response.strip():
                raise Exception("LLMè¿”å›ç©ºå›ç­”")

            return response.strip()

        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]], device_context: str = "") -> str:
        """
        ç®€å•å›ç­”æ¨¡å¼ - åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æ¡£å—
            device_context: è®¾å¤‡ä¿¡æ¯ä¸Šä¸‹æ–‡
            
        Returns:
            str: ç®€å•æ¨¡å¼ç”Ÿæˆçš„å›ç­”
        """
        try:
            answer_parts = []
            print(f"ç®€å•å›ç­”æ¨¡å¼ - è®¾å¤‡ä¸Šä¸‹æ–‡: {device_context}")
            print(f"ç®€å•å›ç­”æ¨¡å¼ - æ–‡æ¡£å—æ•°é‡: {len(context_chunks) if context_chunks else 0}")
            
            # æ·»åŠ è®¾å¤‡ä¿¡æ¯
            if device_context and device_context != "æ— ç›¸å…³è®¾å¤‡ä¿¡æ¯":
                print(f"æ·»åŠ è®¾å¤‡ä¿¡æ¯åˆ°å›ç­”ä¸­")
                answer_parts.append(f"ğŸ“± **è®¾å¤‡ä¿¡æ¯ï¼š**\n{device_context}")
            else:
                print(f"è®¾å¤‡ä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–ä¸º'æ— ç›¸å…³è®¾å¤‡ä¿¡æ¯'ï¼Œè·³è¿‡è®¾å¤‡ä¿¡æ¯")
            
            # æ·»åŠ æ–‡æ¡£ä¿¡æ¯
            if context_chunks:
                print(f"æ·»åŠ æ–‡æ¡£ä¿¡æ¯åˆ°å›ç­”ä¸­")
                best_chunk = context_chunks[0]
                document_name = best_chunk['document'].original_filename or best_chunk['document'].filename
                content = best_chunk['chunk'].content
                answer_parts.append(f"ğŸ“„ **åŸºäºæ–‡æ¡£ã€Š{document_name}ã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼š**\n\n{content}")
            else:
                print(f"æ²¡æœ‰æ–‡æ¡£å—ï¼Œè·³è¿‡æ–‡æ¡£ä¿¡æ¯")
            
            print(f"å›ç­”éƒ¨åˆ†æ•°é‡: {len(answer_parts)}")
            
            if not answer_parts:
                print(f"æ²¡æœ‰å›ç­”éƒ¨åˆ†ï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯")
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æˆ–è®¾å¤‡ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            
            answer = "\n\n" + "\n\n".join(answer_parts)
            answer += "\n\nğŸ’¡ **æç¤º**ï¼šå½“å‰ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼ã€‚é…ç½®OpenAI API Keyåå¯è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"

            return answer

        except Exception:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"


rag_generator = RAGGenerator()
