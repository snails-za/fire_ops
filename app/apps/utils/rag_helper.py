"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—

è¯¥æ¨¡å—åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
1. DocumentProcessor: æ–‡æ¡£å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–
2. VectorSearch: å‘é‡æœç´¢å¼•æ“ï¼ŒåŸºäºChromaæ•°æ®åº“è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
3. RAGGenerator: RAGç”Ÿæˆå™¨ï¼Œé›†æˆLangChainå’ŒOpenAIè¿›è¡Œæ™ºèƒ½é—®ç­”

æŠ€æœ¯æ ˆï¼š
- æ–‡æ¡£è§£æ: ç‹¬ç«‹çš„DocumentParseræ¨¡å—
- æ–‡æœ¬åˆ†å‰²: LangChain RecursiveCharacterTextSplitter
- å‘é‡åŒ–: Sentence Transformers
- å‘é‡å­˜å‚¨: ChromaDB
- æ™ºèƒ½é—®ç­”: LangChain + OpenAI GPT

æ¶æ„è¯´æ˜ï¼š
- æ–‡æ¡£è§£æåŠŸèƒ½å·²æ‹†åˆ†åˆ°ç‹¬ç«‹çš„document_parseræ¨¡å—
- æœ¬æ¨¡å—ä¸“æ³¨äºå‘é‡åŒ–ã€æœç´¢å’Œç”ŸæˆåŠŸèƒ½
- é€šè¿‡ä¾èµ–æ³¨å…¥çš„æ–¹å¼ä½¿ç”¨æ–‡æ¡£è§£æå™¨
"""

import os
import traceback
import uuid
from typing import List, Dict, Any, Optional

import chromadb
from chromadb.config import Settings as ChromaSettings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer

from apps.models.document import Document as DocumentModel, DocumentChunk
from apps.utils.document_parser import document_parser
from config import (
    CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION, EMBEDDING_MODEL,
    HF_HOME, HF_OFFLINE, OPENAI_API_KEY, OPENAI_BASE_URL, SIMILARITY_THRESHOLD
)


# RAGç³»ç»Ÿå·¥å…·å‡½æ•°
def get_local_model_path(model_name: str, cache_folder: str) -> Optional[str]:
    """
    è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„
    
    Args:
        model_name: HuggingFaceæ¨¡å‹åç§°
        cache_folder: ç¼“å­˜æ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–None
    """
    # HuggingFaceå°† '/' è½¬æ¢ä¸º '--'
    local_model_name = model_name.replace('/', '--')
    local_model_path = os.path.join(cache_folder, f"models--{local_model_name}")
    
    if os.path.exists(local_model_path):
        # æ£€æŸ¥æ˜¯å¦æœ‰snapshotsç›®å½•
        snapshots_dir = os.path.join(local_model_path, "snapshots")
        if os.path.exists(snapshots_dir):
            # è·å–æœ€æ–°çš„snapshot
            snapshots = [d for d in os.listdir(snapshots_dir) 
                        if os.path.isdir(os.path.join(snapshots_dir, d))]
            if snapshots:
                latest_snapshot = os.path.join(snapshots_dir, snapshots[0])
                return latest_snapshot
        
        # å¦‚æœæ²¡æœ‰snapshotsï¼Œç›´æ¥è¿”å›æ¨¡å‹ç›®å½•
        return local_model_path
    
    return None


class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†å™¨ - è´Ÿè´£æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    2. ç”Ÿæˆé«˜è´¨é‡å‘é‡åµŒå…¥
    3. ä¸æ•°æ®åº“å’Œå‘é‡å­˜å‚¨åŒæ­¥
    4. åè°ƒæ–‡æ¡£è§£æå™¨å’Œå‘é‡åŒ–æµç¨‹
    
    å¤„ç†æµç¨‹ï¼š
    1. ä½¿ç”¨DocumentParseræå–æ–‡æ¡£å†…å®¹
    2. æ™ºèƒ½åˆ†å—å¤„ç†
    3. ç”Ÿæˆå‘é‡åµŒå…¥
    4. å­˜å‚¨åˆ°ChromaDBå’Œæ•°æ®åº“
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        é…ç½®ç»„ä»¶ï¼š
        1. æ–‡æœ¬åˆ†å‰²å™¨ï¼š1000å­—ç¬¦å—å¤§å°ï¼Œ200å­—ç¬¦é‡å 
        2. å‘é‡åµŒå…¥æ¨¡å‹ï¼šSentence Transformers
        3. ChromaDBå‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        4. æ–‡æ¡£è§£æå™¨ï¼šç‹¬ç«‹çš„DocumentParserå®ä¾‹
        """
        try:
            # é…ç½®æ–‡æœ¬åˆ†å‰²å™¨ - å¹³è¡¡å—å¤§å°å’Œè¯­ä¹‰å®Œæ•´æ€§
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,      # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
                chunk_overlap=200,    # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
                length_function=len,  # ä½¿ç”¨å­—ç¬¦é•¿åº¦è®¡ç®—
            )
            
            # é…ç½®HuggingFaceç¯å¢ƒå˜é‡
            os.environ["HF_HOME"] = HF_HOME
            os.environ["TRANSFORMERS_CACHE"] = HF_HOME
            os.environ["HF_HUB_CACHE"] = HF_HOME
            
            if HF_OFFLINE:
                # ç¦»çº¿æ¨¡å¼é…ç½®
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"
            
            # å…³é—­Chromaé¥æµ‹ï¼Œé¿å…ç½‘ç»œè¯·æ±‚å’Œé”™è¯¯
            os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
            os.environ.setdefault("CHROMA_TELEMETRY", "false")
            
            # åˆå§‹åŒ–å‘é‡åµŒå…¥æ¨¡å‹
            local_model_path = get_local_model_path(EMBEDDING_MODEL, HF_HOME)
            
            if local_model_path and HF_OFFLINE:
                self.embedding_model = SentenceTransformer(local_model_path)
            else:
                self.embedding_model = SentenceTransformer(
                    EMBEDDING_MODEL, 
                    cache_folder=HF_HOME
                )
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ
            os.makedirs(CHROMA_PERSIST_DIRECTORY, exist_ok=True)
            self.chroma_client = chromadb.PersistentClient(
                path=CHROMA_PERSIST_DIRECTORY, 
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            self.collection = self.chroma_client.get_or_create_collection(
                name=CHROMA_COLLECTION, 
                metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            )
            
        except Exception as e:
            raise Exception(f"DocumentProcessoråˆå§‹åŒ–å¤±è´¥: {e}")
        
    async def process_document(self, document_id: int, file_path: str, file_type: str) -> bool:
        """
        å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡åµŒå…¥
        
        å¤„ç†æµç¨‹ï¼š
        1. æå–æ–‡æ¡£å†…å®¹
        2. æ™ºèƒ½åˆ†å—å¤„ç†
        3. ç”Ÿæˆå‘é‡åµŒå…¥
        4. å­˜å‚¨åˆ°ChromaDB
        5. æ›´æ–°æ•°æ®åº“çŠ¶æ€
        
        Args:
            document_id: æ–‡æ¡£ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        document = None
        try:
            # 1. æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤„ç†ä¸­
            document = await DocumentModel.get(id=document_id)
            document.status = "processing"
            await document.save()
            
            # 2. æå–æ–‡æ¡£å†…å®¹
            content = await document_parser.extract_content(file_path, file_type)
            if not content or not content.strip():
                raise Exception("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–")
            
            # æ›´æ–°æ–‡æ¡£å†…å®¹åˆ°æ•°æ®åº“
            document.content = content
            await document.save()
            
            # 3. æ™ºèƒ½åˆ†å—å¤„ç†
            chunks = self.text_splitter.split_text(content)
            if not chunks:
                raise Exception("æ–‡æ¡£åˆ†å—å¤±è´¥")
            
            # 4. åˆ›å»ºåˆ†å—è®°å½•
            chunk_objects = []
            for i, chunk_text in enumerate(chunks):
                chunk = await DocumentChunk.create(
                    document_id=document_id,
                    chunk_index=i,
                    content=chunk_text,
                    content_length=len(chunk_text),
                    metadata={"chunk_index": i}
                )
                chunk_objects.append(chunk)
            
            # 5. ç”Ÿæˆå‘é‡åµŒå…¥
            embeddings = self.embedding_model.encode(chunks)
            
            # 6. å­˜å‚¨å‘é‡åˆ°ChromaDB
            if len(chunk_objects) > 0:
                ids = []
                metadatas = []
                vectors = []
                
                for i, (chunk, embedding) in enumerate(zip(chunk_objects, embeddings)):
                    vector_id = f"doc_{document_id}_chunk_{i}_{uuid.uuid4().hex[:8]}"
                    ids.append(vector_id)
                    metadatas.append({
                        "document_id": document_id,
                        "chunk_id": chunk.id,
                        "chunk_index": i,
                    })
                    vectors.append(embedding.tolist())
                
                # æ‰¹é‡æ·»åŠ åˆ°ChromaDB
                self.collection.add(
                    ids=ids, 
                    embeddings=vectors, 
                    metadatas=metadatas
                )
            
            # 7. æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå®Œæˆ
            document.status = "completed"
            await document.save()
            
            return True
            
        except Exception as e:
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
            try:
                if document is None:
                    document = await DocumentModel.get(id=document_id)
                document.status = "failed"
                document.error_message = str(e)
                await document.save()
            except Exception:
                pass  # å¿½ç•¥ä¿å­˜é”™è¯¯çŠ¶æ€çš„å¼‚å¸¸
            
            return False
    
    


class VectorSearch:
    """
    å‘é‡æœç´¢å¼•æ“ - åŸºäºChromaDBçš„è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    2. å‘é‡æ•°æ®ç®¡ç†
    3. æœç´¢ç»“æœæ’åºå’Œè¿‡æ»¤
    4. ä¸æ•°æ®åº“æ•°æ®å…³è”
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        
        é…ç½®ç»„ä»¶ï¼š
        1. å‘é‡åµŒå…¥æ¨¡å‹ï¼ˆä¸DocumentProcessorä¿æŒä¸€è‡´ï¼‰
        2. ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ
        """
        try:
            # é…ç½®HuggingFaceç¯å¢ƒ
            os.environ["HF_HOME"] = HF_HOME
            os.environ["TRANSFORMERS_CACHE"] = HF_HOME
            os.environ["HF_HUB_CACHE"] = HF_HOME
            
            if HF_OFFLINE:
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"
            
            # å…³é—­Chromaé¥æµ‹
            os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
            os.environ.setdefault("CHROMA_TELEMETRY", "false")
            
            # åˆå§‹åŒ–å‘é‡åµŒå…¥æ¨¡å‹ï¼ˆä¸DocumentProcessorä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼‰
            local_model_path = get_local_model_path(EMBEDDING_MODEL, HF_HOME)
            
            if local_model_path and HF_OFFLINE:
                self.embedding_model = SentenceTransformer(local_model_path)
            else:
                self.embedding_model = SentenceTransformer(
                    EMBEDDING_MODEL, 
                    cache_folder=HF_HOME
                )
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(
                path=CHROMA_PERSIST_DIRECTORY, 
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            self.collection = self.chroma_client.get_or_create_collection(
                name=CHROMA_COLLECTION, 
                metadata={"hnsw:space": "cosine"}
            )
            
        except Exception as e:
            raise Exception(f"VectorSearchåˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def search_similar_chunks(self, query: str, top_k: int = 5, use_threshold: bool = True) -> List[Dict[str, Any]]:
        """
        æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£å—
        
        æœç´¢æµç¨‹ï¼š
        1. å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        2. åœ¨ChromaDBä¸­è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
        3. ä»æ•°æ®åº“è·å–å®Œæ•´çš„æ–‡æ¡£å’Œå—ä¿¡æ¯
        4. è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°å¹¶æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            
        Returns:
            List[Dict]: ç›¸ä¼¼æ–‡æ¡£å—åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æ¡£ã€å—å’Œç›¸ä¼¼åº¦ä¿¡æ¯
        """
        try:
            if not query or not query.strip():
                return []
            
            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode([query.strip()])[0]
            
            # 2. åœ¨ChromaDBä¸­æœç´¢ç›¸ä¼¼å‘é‡
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()], 
                n_results=min(top_k, 20)  # é™åˆ¶æœ€å¤§æœç´¢æ•°é‡
            )
            
            # 3. è§£ææœç´¢ç»“æœ
            ids = results.get('ids', [[]])[0]
            metadatas = results.get('metadatas', [[]])[0]
            distances = results.get('distances', [[]])[0] or []
            
            # 4. ä»æ•°æ®åº“è·å–å®Œæ•´ä¿¡æ¯å¹¶æ„å»ºç»“æœ
            all_similarities = []  # å­˜å‚¨æ‰€æœ‰ç»“æœ
            filtered_similarities = []  # å­˜å‚¨è¿‡æ»¤åçš„ç»“æœ
            
            for i, (vector_id, metadata) in enumerate(zip(ids, metadatas)):
                try:
                    # è·å–æ–‡æ¡£å—ä¿¡æ¯
                    chunk_id = metadata.get('chunk_id')
                    if not chunk_id:
                        continue
                    
                    chunk = await DocumentChunk.get_or_none(id=chunk_id)
                    if not chunk:
                        continue
                    
                    # è·å–å…³è”çš„æ–‡æ¡£
                    document = await chunk.document
                    if not document:
                        continue
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
                    distance = distances[i] if i < len(distances) else 1.0
                    similarity = max(0.0, 1.0 - float(distance))
                    
                    result_item = {
                        'vector_id': vector_id,
                        'similarity': similarity,
                        'chunk': chunk,
                        'document': document,
                        'metadata': metadata,
                        'above_threshold': similarity >= SIMILARITY_THRESHOLD
                    }
                    
                    all_similarities.append(result_item)
                    
                    # å¦‚æœä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåªä¿ç•™ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ç»“æœ
                    if not use_threshold or similarity >= SIMILARITY_THRESHOLD:
                        filtered_similarities.append(result_item)
                    
                except Exception as e:
                    print(f"å¤„ç†æœç´¢ç»“æœé¡¹å¤±è´¥ (chunk_id: {metadata.get('chunk_id', 'unknown')}): {e}")
                    continue
            
            # 5. æ™ºèƒ½é€‰æ‹©è¿”å›ç»“æœ
            if use_threshold and filtered_similarities:
                # æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œè¿”å›è¿‡æ»¤åçš„ç»“æœ
                filtered_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                return filtered_similarities[:top_k]
            elif use_threshold and not filtered_similarities and all_similarities:
                # æ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œä½†æœ‰æœç´¢ç»“æœï¼Œè¿”å›æœ€ç›¸ä¼¼çš„å‡ ä¸ªå¹¶æ ‡è®°
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                # å–å‰å‡ ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœï¼Œä½†æ ‡è®°ä¸ºä½ç›¸ä¼¼åº¦
                return all_similarities[:min(top_k, 3)]  # æœ€å¤šè¿”å›3ä¸ªä½ç›¸ä¼¼åº¦ç»“æœ
            else:
                # ä¸ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œè¿”å›æ‰€æœ‰ç»“æœ
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                return all_similarities[:top_k]
            
        except Exception as e:
            print(f"æœç´¢ç›¸ä¼¼æ–‡æ¡£å—å¤±è´¥: {e}")
            traceback.print_exc()
            return []

    async def delete_document_vectors(self, document_id: int):
        """
        åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡æ•°æ®
        
        Args:
            document_id: æ–‡æ¡£ID
        """
        try:
            # é€šè¿‡metadataæ¡ä»¶åˆ é™¤
            self.collection.delete(where={"document_id": document_id})
            
        except Exception as e:
            raise Exception(f"åˆ é™¤æ–‡æ¡£ {document_id} å‘é‡æ•°æ®å¤±è´¥: {e}")

    async def count_vectors(self) -> int:
        """
        ç»Ÿè®¡å‘é‡æ•°æ®åº“ä¸­çš„å‘é‡æ€»æ•°
        
        Returns:
            int: å‘é‡æ€»æ•°
        """
        try:
            count = self.collection.count()
            return int(count) if isinstance(count, (int, float)) else 0
            
        except Exception:
            return 0


class RAGGenerator:
    """
    RAGç”Ÿæˆå™¨ - æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. é›†æˆå‘é‡æœç´¢å’ŒLLMç”Ÿæˆ
    2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
    3. æ”¯æŒå¤šç§å›ç­”æ¨¡å¼ï¼ˆLLMæ™ºèƒ½å›ç­” / ç®€å•å›ç­”ï¼‰
    4. æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—®ç­”ä½“éªŒ
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–RAGç”Ÿæˆå™¨
        
        ç»„ä»¶ï¼š
        1. VectorSearch: å‘é‡æœç´¢å¼•æ“
        2. LangChain + OpenAI: æ™ºèƒ½é—®ç­”é“¾ï¼ˆå¯é€‰ï¼‰
        3. å¤‡ç”¨ç®€å•å›ç­”æ¨¡å¼
        """
        try:
            # åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
            self.vector_search = VectorSearch()
            
            # åˆå§‹åŒ–LLMç»„ä»¶
            self.llm = None
            self.chain = None
            self.llm_available = False
            
            # æ£€æŸ¥OpenAIé…ç½®å¹¶åˆå§‹åŒ–LLM
            if OPENAI_API_KEY and OPENAI_API_KEY.strip():
                try:
                    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
                    self.llm = ChatOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿å›ç­”çš„ä¸€è‡´æ€§
                        model="gpt-3.5-turbo",
                        max_tokens=2000  # é™åˆ¶å›ç­”é•¿åº¦
                    )
                    
                    # åˆ›å»ºç³»ç»Ÿæç¤ºæ¨¡æ¿
                    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„ï¼š
   - ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜ç»„ç»‡å†…å®¹
   - é‡è¦ä¿¡æ¯ç”¨**ç²—ä½“**æ ‡è®°
   - ä½¿ç”¨é¡¹ç›®ç¬¦å·(â€¢)æˆ–æ•°å­—åˆ—è¡¨å±•ç¤ºè¦ç‚¹
   - æ¯ä¸ªæ®µè½ä¸“æ³¨ä¸€ä¸ªä¸»é¢˜ï¼Œæ®µè½é—´ç•™ç©ºè¡Œ
   - å¤æ‚å†…å®¹ä½¿ç”¨è¡¨æ ¼æˆ–ç»“æ„åŒ–æ ¼å¼
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œå…³é”®å†…å®¹ç‰‡æ®µï¼Œæ ¼å¼ï¼šã€Œæ–‡æ¡£åï¼šå…·ä½“å†…å®¹ã€
5. å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£æä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶æ ‡æ˜ä¿¡æ¯æ¥æº
6. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚å½“ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}"""

                    human_template = "é—®é¢˜ï¼š{question}"
                    
                    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
                    chat_prompt = ChatPromptTemplate([
                        ("system", system_template),
                        ("human", human_template),
                    ])
                    
                    # åˆ›å»ºè¾“å‡ºè§£æå™¨
                    output_parser = StrOutputParser()
                    
                    # åˆ›å»ºLangChainå¤„ç†é“¾
                    self.chain = chat_prompt | self.llm | output_parser
                    self.llm_available = True
                    
                except Exception:
                    self.llm = None
                    self.chain = None
                    self.llm_available = False
            else:
                self.llm_available = False
                
        except Exception as e:
            raise Exception(f"RAGGeneratoråˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        å›ç­”ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨LLMæ™ºèƒ½å›ç­”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. é™çº§åˆ°ç®€å•å›ç­”æ¨¡å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        3. æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ¥æºå¼•ç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                content = chunk['chunk'].content
                similarity = chunk['similarity']
                
                # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
                context_part = f"""æ–‡æ¡£ {i}: {doc_name}
ç›¸ä¼¼åº¦: {similarity:.2f}
å†…å®¹: {content}"""
                context_parts.append(context_part)
            
            context = "\n\n" + "="*50 + "\n\n".join(context_parts)
            
            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    answer = await self._llm_answer(query, context)
                    
                    # ä¸å†è‡ªåŠ¨æ·»åŠ æ¥æºä¿¡æ¯ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦æ˜¾ç¤º
                    return answer
                    
                except Exception:
                    return self._simple_answer(query, context_chunks)
            else:
                return self._simple_answer(query, context_chunks)
                
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    async def generate_answer_stream(self, query: str, context_chunks: List[Dict[str, Any]]):
        """
        æµå¼ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ–‡æ¡£ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk.get('document_name', 'æœªçŸ¥æ–‡æ¡£')
                content = chunk.get('content', '')
                context_parts.append(f"æ–‡æ¡£{i}: {doc_name}\nå†…å®¹: {content}")
            
            context = "\n\n" + "="*50 + "\n\n".join(context_parts)
            
            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    async for chunk in self._llm_answer_stream(query, context):
                        yield chunk
                except Exception as e:
                    print(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {e}")
                    # é™çº§åˆ°ç®€å•å›ç­”
                    simple_answer = self._simple_answer(query, context_chunks)
                    yield simple_answer
            else:
                # éLLMæ¨¡å¼ï¼Œç›´æ¥è¿”å›ç®€å•å›ç­”
                simple_answer = self._simple_answer(query, context_chunks)
                yield simple_answer
                
        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def _llm_answer_stream(self, query: str, context: str):
        """
        ä½¿ç”¨LangChainé“¾å¼æµå¼ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„chainè¿›è¡Œæµå¼è°ƒç”¨
            if self.chain:
                async for chunk in self.chain.astream({
                    "question": query,
                    "context": context
                }):
                    if chunk:
                        yield chunk
            else:
                raise Exception("LangChainå¤„ç†é“¾æœªåˆå§‹åŒ–")
                    
        except Exception as e:
            traceback.print_exc()
            raise Exception(f"LLMé“¾å¼æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
    
    async def _llm_answer(self, query: str, context: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Returns:
            str: LLMç”Ÿæˆçš„å›ç­”
        """
        try:
            # è°ƒç”¨LangChainå¤„ç†é“¾
            response = await self.chain.ainvoke({
                "question": query,
                "context": context
            })
            
            if not response or not response.strip():
                raise Exception("LLMè¿”å›ç©ºå›ç­”")
            
            return response.strip()
            
        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        ç®€å•å›ç­”æ¨¡å¼ - åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç®€å•æ¨¡å¼ç”Ÿæˆçš„å›ç­”
        """
        try:
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            
            # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
            best_chunk = context_chunks[0]
            document_name = best_chunk['document'].original_filename or best_chunk['document'].filename
            content = best_chunk['chunk'].content
            similarity = best_chunk['similarity']
            
            # æ„å»ºç®€å•å›ç­”ï¼ˆä¸åŒ…å«å‚è€ƒæ¥æºï¼‰
            answer = f"""åŸºäºæ–‡æ¡£ã€Š{document_name}ã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼š

{content}

ğŸ’¡ **æç¤º**ï¼šå½“å‰ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼ã€‚é…ç½®OpenAI API Keyåå¯è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"""
            
            return answer
            
        except Exception:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def _build_sources_info(self, context_chunks: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºæ¥æºä¿¡æ¯
        
        Args:
            context_chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ¥æºä¿¡æ¯
        """
        try:
            sources = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                similarity = chunk['similarity']
                sources.append(f"â€¢ {doc_name} (ç›¸ä¼¼åº¦: {similarity:.1%})")
            
            sources_text = "\n".join(sources)
            return f"""---
ğŸ“‹ **å‚è€ƒæ¥æº**ï¼š
{sources_text}

ğŸ’¡ åŸºäº {len(context_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µç”Ÿæˆæ­¤å›ç­”"""
            
        except Exception:
            return "---\nğŸ“‹ **å‚è€ƒæ¥æº**ï¼šä¿¡æ¯è·å–å¤±è´¥"


# å…¨å±€å®ä¾‹ - å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿æ•´ä¸ªåº”ç”¨ä½¿ç”¨ç›¸åŒçš„å®ä¾‹
try:
    # æ–‡æ¡£å¤„ç†å™¨å®ä¾‹
    document_processor = DocumentProcessor()
    
    # å‘é‡æœç´¢å®ä¾‹
    vector_search = VectorSearch()
    
    # RAGç”Ÿæˆå™¨å®ä¾‹
    rag_generator = RAGGenerator()
    
except Exception as e:
    raise Exception(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
