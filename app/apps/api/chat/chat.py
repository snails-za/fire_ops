import traceback
from typing import Optional

from fastapi import APIRouter, Query, Form
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from apps.utils import response
from apps.utils.rag_helper import vector_search, rag_generator
from config import OPENAI_API_KEY, OPENAI_BASE_URL

router = APIRouter(prefix="/chat", tags=["æ™ºèƒ½é—®ç­”"])

# åˆå§‹åŒ–LLMç”¨äºŽé—®é¢˜ç†è§£å’Œä¼˜åŒ–
question_optimizer = None
search_optimizer = None

if OPENAI_API_KEY and OPENAI_API_KEY.strip():
    try:
        question_llm = ChatOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL,
            temperature=0.1,
            model="gpt-3.5-turbo"
        )
        
        # é—®é¢˜ç†è§£å’Œä¼˜åŒ–æç¤ºæ¨¡æ¿
        question_template = """ä½ æ˜¯ä¸€ä¸ªé—®é¢˜ç†è§£åŠ©æ‰‹ã€‚è¯·åˆ†æžç”¨æˆ·çš„é—®é¢˜ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ä¼˜åŒ–æœç´¢ç­–ç•¥ã€‚

ä»»åŠ¡ï¼š
1. ç†è§£é—®é¢˜çš„æ ¸å¿ƒæ„å›¾å’Œå…³é”®æ¦‚å¿µ
2. è¯†åˆ«é—®é¢˜ç±»åž‹ï¼ˆäº‹å®žæŸ¥è¯¢ã€æ“ä½œæŒ‡å¯¼ã€æ¦‚å¿µè§£é‡Šç­‰ï¼‰
3. æå–æœ€é‡è¦çš„æœç´¢å…³é”®è¯
4. å¦‚æžœé—®é¢˜æ¨¡ç³Šï¼ŒæŽ¨æµ‹å¯èƒ½çš„å…·ä½“å«ä¹‰

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç”¨ä»¥ä¸‹JSONæ ¼å¼å›žç­”ï¼š
{{
    "intent": "é—®é¢˜æ„å›¾æè¿°",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
    "question_type": "é—®é¢˜ç±»åž‹",
    "optimized_query": "ä¼˜åŒ–åŽçš„æœç´¢æŸ¥è¯¢"
}}"""
        
        # æœç´¢æŸ¥è¯¢ä¼˜åŒ–æ¨¡æ¿
        search_template = """ä½ æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºæœ€é€‚åˆæ–‡æ¡£æœç´¢çš„æŸ¥è¯¢è¯­å¥ã€‚

è¦æ±‚ï¼š
1. æå–æ ¸å¿ƒå…³é”®è¯å’Œæ¦‚å¿µ
2. åŽ»é™¤æ— å…³çš„è¯­æ°”è¯å’Œä¿®é¥°è¯
3. ä¿æŒæŸ¥è¯¢çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
4. é€‚åˆå‘é‡ç›¸ä¼¼åº¦æœç´¢

åŽŸé—®é¢˜ï¼š{question}

ä¼˜åŒ–åŽçš„æœç´¢æŸ¥è¯¢ï¼ˆåªè¾“å‡ºæŸ¥è¯¢è¯­å¥ï¼‰ï¼š"""
        
        question_prompt = ChatPromptTemplate.from_template(question_template)
        search_prompt = ChatPromptTemplate.from_template(search_template)
        output_parser = StrOutputParser()
        
        question_optimizer = question_prompt | question_llm | output_parser
        search_optimizer = search_prompt | question_llm | output_parser
        
        print("âœ… é—®é¢˜ç†è§£å’Œæœç´¢ä¼˜åŒ–LLMåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ é—®é¢˜ä¼˜åŒ–LLMåˆå§‹åŒ–å¤±è´¥: {e}")
        question_optimizer = None
        search_optimizer = None


@router.post("/ask", summary="æ™ºèƒ½é—®ç­”(åŒ¿å)", description="åŸºäºŽLLMçš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ï¼ˆæ— éœ€ç™»å½•ï¼‰")
async def ask_question_anonymous(
    question: str = Form(..., description="ç”¨æˆ·é—®é¢˜"),
    top_k: int = Form(5, ge=1, le=10, description="æ£€ç´¢ç›¸å…³æ–‡æ¡£æ•°é‡"),
):
    """åŒ¿åæ™ºèƒ½é—®ç­” - é›†æˆLLMé—®é¢˜ç†è§£å’Œæœç´¢ä¼˜åŒ–"""
    try:
        original_question = question.strip()
        search_query = original_question
        question_analysis = None
        
        # ç¬¬ä¸€æ­¥ï¼šLLMé—®é¢˜ç†è§£å’Œæœç´¢ä¼˜åŒ–
        if search_optimizer and len(original_question) > 2:
            try:
                print(f"ðŸ” åŽŸå§‹é—®é¢˜: {original_question}")
                
                # ä¼˜åŒ–æœç´¢æŸ¥è¯¢
                optimized_query = await search_optimizer.ainvoke({
                    "question": original_question
                })
                optimized_query = optimized_query.strip()
                
                # éªŒè¯ä¼˜åŒ–ç»“æžœ
                if len(optimized_query) >= 2 and len(optimized_query) <= len(original_question) * 2:
                    search_query = optimized_query
                    print(f"âœ¨ ä¼˜åŒ–æœç´¢: {search_query}")
                else:
                    search_query = original_question
                    print(f"âš ï¸ æœç´¢ä¼˜åŒ–æ— æ•ˆï¼Œä½¿ç”¨åŽŸé—®é¢˜")
                    
            except Exception as e:
                print(f"æœç´¢ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŽŸé—®é¢˜: {e}")
                search_query = original_question
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ä¼˜åŒ–åŽçš„æŸ¥è¯¢è¿›è¡Œæ–‡æ¡£æœç´¢
        print(f"ðŸ”Ž æ‰§è¡Œæœç´¢: {search_query}")
        similar_chunks = await vector_search.search_similar_chunks(search_query, top_k)
        
        if not similar_chunks:
            # å¦‚æžœä¼˜åŒ–æŸ¥è¯¢æ²¡æœ‰ç»“æžœï¼Œå°è¯•åŽŸé—®é¢˜
            if search_query != original_question:
                print(f"ðŸ”„ ä¼˜åŒ–æŸ¥è¯¢æ— ç»“æžœï¼Œå°è¯•åŽŸé—®é¢˜: {original_question}")
                similar_chunks = await vector_search.search_similar_chunks(original_question, top_k)
        
        # ç¬¬ä¸‰æ­¥ï¼šLLMç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ
        print(f"ðŸ“ ç”Ÿæˆç­”æ¡ˆï¼Œæ‰¾åˆ° {len(similar_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        answer = await rag_generator.generate_answer(original_question, similar_chunks)
        
        # æž„å»ºå“åº”æ•°æ®
        response_data = {
            "question": original_question,
            "answer": answer,
            "sources": [
                {
                    "document_id": chunk["document"].id,
                    "document_name": chunk["document"].filename,
                    "original_filename": chunk["document"].original_filename,
                    "file_type": chunk["document"].file_type,
                    "chunk_id": chunk["chunk"].id,
                    "chunk_index": chunk["chunk"].chunk_index,
                    "similarity": chunk["similarity"],
                    "content_preview": chunk["chunk"].content[:200] + "..." if len(chunk["chunk"].content) > 200 else chunk["chunk"].content,
                    "full_content": chunk["chunk"].content,
                    "download_url": f"/api/v1/documents/{chunk['document'].id}/download",
                    "view_url": f"/api/v1/documents/{chunk['document'].id}/view?chunk_id={chunk['chunk'].id}",
                    "highlight_url": f"/api/v1/documents/{chunk['document'].id}/view?chunk_id={chunk['chunk'].id}&highlight="
                }
                for chunk in similar_chunks
            ],
            "search_info": {
                "original_query": original_question,
                "search_query": search_query,
                "results_count": len(similar_chunks),
                "llm_enhanced": search_optimizer is not None
            }
        }
        
        return response(data=response_data)
        
    except Exception as e:
        print(f"é—®ç­”å¤„ç†å¼‚å¸¸: {e}")
        traceback.print_exc()
        return response(code=500, message=f"é—®ç­”å¤±è´¥: {str(e)}")


@router.get("/search", summary="æ–‡æ¡£æœç´¢(åŒ¿å)", description="åŸºäºŽLLMä¼˜åŒ–çš„æ–‡æ¡£æœç´¢ï¼ˆæ— éœ€ç™»å½•ï¼‰")
async def search_documents(
    query: str,
    top_k: int = Query(5, ge=1, le=20, description="è¿”å›žç»“æžœæ•°é‡"),
):
    """æœç´¢ç›¸å…³æ–‡æ¡£ - é›†æˆLLMæŸ¥è¯¢ä¼˜åŒ–"""
    try:
        original_query = query.strip()
        search_query = original_query
        
        # LLMä¼˜åŒ–æœç´¢æŸ¥è¯¢
        if search_optimizer and len(original_query) > 2:
            try:
                optimized_query = await search_optimizer.ainvoke({
                    "question": original_query
                })
                optimized_query = optimized_query.strip()
                
                if len(optimized_query) >= 2 and len(optimized_query) <= len(original_query) * 2:
                    search_query = optimized_query
                    
            except Exception as e:
                print(f"æœç´¢ä¼˜åŒ–å¤±è´¥: {e}")
        
        # æ‰§è¡Œæœç´¢
        similar_chunks = await vector_search.search_similar_chunks(search_query, top_k)
        
        # å¦‚æžœä¼˜åŒ–æŸ¥è¯¢æ— ç»“æžœï¼Œå°è¯•åŽŸæŸ¥è¯¢
        if not similar_chunks and search_query != original_query:
            similar_chunks = await vector_search.search_similar_chunks(original_query, top_k)
        
        results = []
        for chunk in similar_chunks:
            results.append({
                "document_id": chunk["document"].id,
                "document_name": chunk["document"].filename,
                "chunk_content": chunk["chunk"].content,
                "similarity": chunk["similarity"],
                "chunk_index": chunk["chunk"].chunk_index
            })
        
        return response(data={
            "query": original_query,
            "search_query": search_query,
            "results": results,
            "total": len(results),
            "llm_enhanced": search_optimizer is not None
        })
        
    except Exception as e:
        traceback.print_exc()
        return response(code=500, message=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/analyze", summary="é—®é¢˜åˆ†æž(åŒ¿å)", description="ä½¿ç”¨LLMåˆ†æžé—®é¢˜æ„å›¾å’Œå…³é”®è¯ï¼ˆæ— éœ€ç™»å½•ï¼‰")
async def analyze_question(
    question: str = Form(..., description="ç”¨æˆ·é—®é¢˜"),
):
    """é—®é¢˜åˆ†æž - å±•ç¤ºLLMçš„é—®é¢˜ç†è§£èƒ½åŠ›"""
    try:
        if not question_optimizer:
            return response(data={
                "question": question,
                "analysis": "LLMæœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæ·±åº¦åˆ†æž",
                "llm_available": False
            })
        
        # ä½¿ç”¨LLMåˆ†æžé—®é¢˜
        analysis_result = await question_optimizer.ainvoke({
            "question": question
        })
        
        return response(data={
            "question": question,
            "analysis": analysis_result,
            "llm_available": True
        })
        
    except Exception as e:
        traceback.print_exc()
        return response(code=500, message=f"é—®é¢˜åˆ†æžå¤±è´¥: {str(e)}")