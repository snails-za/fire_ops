# RAGç³»ç»ŸæŠ€æœ¯æ–‡æ¡£

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [æŠ€æœ¯æ¶æ„](#æŠ€æœ¯æ¶æ„)
3. [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
4. [APIæ¥å£](#apiæ¥å£)
5. [å‰ç«¯ç•Œé¢](#å‰ç«¯ç•Œé¢)
6. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
7. [éƒ¨ç½²æŒ‡å—](#éƒ¨ç½²æŒ‡å—)
8. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
9. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
10. [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)

---

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

### ä»€ä¹ˆæ˜¯RAGç³»ç»Ÿï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIæŠ€æœ¯ã€‚æœ¬ç³»ç»Ÿå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„RAGè§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿï¼š

- **æ™ºèƒ½æ–‡æ¡£å¤„ç†**ï¼šæ”¯æŒPDFã€DOCXã€Excelã€TXTç­‰å¤šç§æ ¼å¼
- **è¯­ä¹‰æœç´¢**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ™ºèƒ½æ£€ç´¢
- **æ™ºèƒ½é—®ç­”**ï¼šç»“åˆLLMçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›ç­”
- **é«˜äº®æ˜¾ç¤º**ï¼šæ™ºèƒ½å…³é”®è¯é«˜äº®å’Œæ–‡æ¡£æŸ¥çœ‹

### æ ¸å¿ƒç‰¹æ€§

âœ… **å¤šæ ¼å¼æ–‡æ¡£æ”¯æŒ** - PDFã€DOCXã€XLSXã€TXT  
âœ… **æ™ºèƒ½æ–‡æœ¬åˆ†å—** - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„æ–‡æ¡£åˆ†å‰²  
âœ… **å‘é‡åŒ–å­˜å‚¨** - åŸºäºSentence Transformersçš„é«˜è´¨é‡åµŒå…¥  
âœ… **è¯­ä¹‰æœç´¢** - ChromaDBå‘é‡æ•°æ®åº“æ”¯æŒ  
âœ… **LLMé›†æˆ** - OpenAI GPTæ™ºèƒ½é—®ç­”  
âœ… **å®æ—¶å¤„ç†** - å¼‚æ­¥æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–  
âœ… **ç”¨æˆ·å‹å¥½** - ç°ä»£åŒ–Webç•Œé¢å’Œäº¤äº’ä½“éªŒ  

---

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å‰ç«¯ç•Œé¢      â”‚    â”‚   FastAPIåç«¯   â”‚    â”‚   æ•°æ®å­˜å‚¨      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ èŠå¤©ç•Œé¢      â”‚â—„â”€â”€â–ºâ”‚ â€¢ REST API      â”‚â—„â”€â”€â–ºâ”‚ â€¢ PostgreSQL    â”‚
â”‚ â€¢ æ–‡æ¡£ç®¡ç†      â”‚    â”‚ â€¢ å¼‚æ­¥å¤„ç†      â”‚    â”‚ â€¢ ChromaDB      â”‚
â”‚ â€¢ é«˜äº®æ˜¾ç¤º      â”‚    â”‚ â€¢ é”™è¯¯å¤„ç†      â”‚    â”‚ â€¢ æ–‡ä»¶ç³»ç»Ÿ      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   RAGæ ¸å¿ƒç»„ä»¶   â”‚
                       â”‚                 â”‚
                       â”‚ â€¢ DocumentProcessor â”‚
                       â”‚ â€¢ VectorSearch  â”‚
                       â”‚ â€¢ RAGGenerator  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   å¤–éƒ¨æœåŠ¡      â”‚
                       â”‚                 â”‚
                       â”‚ â€¢ OpenAI API    â”‚
                       â”‚ â€¢ HuggingFace   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯æ ˆ

**åç«¯æ¡†æ¶**
- FastAPI 0.115.5 - ç°ä»£Python Webæ¡†æ¶
- Tortoise-ORM - å¼‚æ­¥ORM
- Pydantic - æ•°æ®éªŒè¯

**AI/MLç»„ä»¶**
- LangChain - LLMåº”ç”¨æ¡†æ¶
- Sentence Transformers - æ–‡æœ¬å‘é‡åŒ–
- OpenAI GPT - æ™ºèƒ½é—®ç­”
- ChromaDB - å‘é‡æ•°æ®åº“

**æ–‡æ¡£å¤„ç†**
- PyPDF - PDFæ–‡æœ¬æå–
- python-docx - Wordæ–‡æ¡£å¤„ç†
- openpyxl - Excelæ–‡ä»¶å¤„ç†

**æ•°æ®å­˜å‚¨**
- PostgreSQL - å…³ç³»å‹æ•°æ®åº“
- Redis - ç¼“å­˜å’Œä¼šè¯
- æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ - æ–‡æ¡£å­˜å‚¨

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. DocumentProcessorï¼ˆæ–‡æ¡£å¤„ç†å™¨ï¼‰

**åŠŸèƒ½èŒè´£**
- å¤šæ ¼å¼æ–‡æ¡£å†…å®¹æå–
- æ™ºèƒ½æ–‡æœ¬åˆ†å—
- å‘é‡åµŒå…¥ç”Ÿæˆ
- æ•°æ®åº“åŒæ­¥

**å…³é”®æ–¹æ³•**
```python
class DocumentProcessor:
    async def process_document(self, document_id: int, file_path: str, file_type: str) -> bool
    async def _extract_content(self, file_path: str, file_type: str) -> str
    async def _extract_pdf_content(self, file_path: str) -> str
    async def _extract_docx_content(self, file_path: str) -> str
    async def _extract_excel_content(self, file_path: str) -> str
    async def _extract_txt_content(self, file_path: str) -> str
```

**å¤„ç†æµç¨‹**
1. **æ–‡æ¡£ä¸Šä¼ ** â†’ ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
2. **å†…å®¹æå–** â†’ æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
3. **æ–‡æœ¬åˆ†å—** â†’ ä½¿ç”¨RecursiveCharacterTextSplitter
4. **å‘é‡åŒ–** â†’ Sentence Transformersç”ŸæˆåµŒå…¥
5. **å­˜å‚¨** â†’ ä¿å­˜åˆ°ChromaDBå’ŒPostgreSQL

**é…ç½®å‚æ•°**
```python
# æ–‡æœ¬åˆ†å—é…ç½®
chunk_size = 1000      # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
chunk_overlap = 200    # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

# å‘é‡æ¨¡å‹é…ç½®
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
```

### 2. VectorSearchï¼ˆå‘é‡æœç´¢å¼•æ“ï¼‰

**åŠŸèƒ½èŒè´£**
- è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
- å‘é‡æ•°æ®ç®¡ç†
- æœç´¢ç»“æœæ’åº
- æ•°æ®åº“å…³è”æŸ¥è¯¢

**å…³é”®æ–¹æ³•**
```python
class VectorSearch:
    async def search_similar_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]
    async def delete_document_vectors(self, document_id: int)
    async def count_vectors(self) -> int
```

**æœç´¢æµç¨‹**
1. **æŸ¥è¯¢å‘é‡åŒ–** â†’ å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºå‘é‡
2. **ç›¸ä¼¼åº¦è®¡ç®—** â†’ ChromaDBä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
3. **ç»“æœè¿‡æ»¤** â†’ éªŒè¯æ•°æ®åº“ä¸­çš„è®°å½•
4. **æ’åºè¿”å›** â†’ æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—

**æ€§èƒ½ä¼˜åŒ–**
- ä½¿ç”¨ChromaDBçš„HNSWç´¢å¼•
- æ‰¹é‡æŸ¥è¯¢å‡å°‘æ•°æ®åº“è®¿é—®
- å¼‚æ­¥å¤„ç†æé«˜å“åº”é€Ÿåº¦

### 3. RAGGeneratorï¼ˆRAGç”Ÿæˆå™¨ï¼‰

**åŠŸèƒ½èŒè´£**
- é›†æˆæ£€ç´¢å’Œç”Ÿæˆ
- LLMæ™ºèƒ½é—®ç­”
- ä¸Šä¸‹æ–‡æ„å»º
- å›ç­”è´¨é‡æ§åˆ¶

**å…³é”®æ–¹æ³•**
```python
class RAGGenerator:
    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str
    async def _llm_answer(self, query: str, context: str) -> str
    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str
    def _build_sources_info(self, context_chunks: List[Dict[str, Any]]) -> str
```

**å›ç­”ç­–ç•¥**
1. **æ™ºèƒ½æ¨¡å¼** â†’ ä½¿ç”¨OpenAI GPTç”Ÿæˆå›ç­”
2. **ç®€å•æ¨¡å¼** â†’ åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ
3. **é™çº§å¤„ç†** â†’ LLMå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°ç®€å•æ¨¡å¼

**æç¤ºå·¥ç¨‹**
```python
system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œå…³é”®å†…å®¹ç‰‡æ®µ
5. å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£æä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æ
6. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}"""
```

---

## ğŸŒ APIæ¥å£

### æ–‡æ¡£ç®¡ç†API

#### 1. ä¸Šä¼ æ–‡æ¡£
```http
POST /api/v1/documents/upload
Content-Type: multipart/form-data

å‚æ•°:
- file: æ–‡æ¡£æ–‡ä»¶ (PDF/DOCX/XLSX/TXT)

å“åº”:
{
    "code": 200,
    "message": "æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­...",
    "data": {
        "id": 1,
        "filename": "document.pdf",
        "original_filename": "åŸå§‹æ–‡æ¡£.pdf",
        "file_size": 1024000,
        "file_type": "pdf",
        "status": "processing"
    }
}
```

#### 2. è·å–æ–‡æ¡£åˆ—è¡¨
```http
GET /api/v1/documents/list?page=1&page_size=10&status=completed

å“åº”:
{
    "code": 200,
    "data": {
        "documents": [...],
        "total": 50,
        "page": 1,
        "page_size": 10,
        "total_pages": 5
    }
}
```

#### 3. æŸ¥çœ‹æ–‡æ¡£å†…å®¹
```http
GET /api/v1/documents/{document_id}/view?chunk_id={chunk_id}&highlight={keywords}

å“åº”:
{
    "code": 200,
    "data": {
        "document": {...},
        "content": "æ–‡æ¡£å†…å®¹",
        "highlighted_content": "é«˜äº®åçš„å†…å®¹",
        "highlight_text": "å…³é”®è¯",
        "has_highlight": true
    }
}
```

### æ™ºèƒ½é—®ç­”API

#### 1. æ™ºèƒ½é—®ç­”
```http
POST /api/v1/chat/ask
Content-Type: application/x-www-form-urlencoded

å‚æ•°:
- question: ç”¨æˆ·é—®é¢˜
- top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡ (é»˜è®¤5)

å“åº”:
{
    "code": 200,
    "data": {
        "question": "ç”¨æˆ·é—®é¢˜",
        "answer": "AIç”Ÿæˆçš„å›ç­”",
        "sources": [
            {
                "document_id": 1,
                "document_name": "æ–‡æ¡£åç§°",
                "similarity": 0.85,
                "content_preview": "å†…å®¹é¢„è§ˆ...",
                "download_url": "/api/v1/documents/1/download",
                "view_url": "/api/v1/documents/1/view?chunk_id=1"
            }
        ],
        "search_info": {
            "original_query": "åŸå§‹é—®é¢˜",
            "search_query": "ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢",
            "results_count": 3,
            "llm_enhanced": true
        }
    }
}
```

#### 2. æ–‡æ¡£æœç´¢
```http
GET /api/v1/chat/search?query=æœç´¢å…³é”®è¯&top_k=5

å“åº”:
{
    "code": 200,
    "data": {
        "query": "æœç´¢å…³é”®è¯",
        "results": [...],
        "total": 3,
        "llm_enhanced": true
    }
}
```

---

## ğŸ¨ å‰ç«¯ç•Œé¢

### èŠå¤©ç•Œé¢ (chat.html)

**æ ¸å¿ƒåŠŸèƒ½**
- å®æ—¶é—®ç­”äº¤äº’
- æ¶ˆæ¯å†å²è®°å½•
- æ¥æºæ–‡æ¡£å±•ç¤º
- æ™ºèƒ½é«˜äº®æŸ¥çœ‹

**å…³é”®ç»„ä»¶**
```javascript
// å‘é€æ¶ˆæ¯
async function sendMessage()

// æ™ºèƒ½é«˜äº®æ˜¾ç¤º
async function viewWithSmartHighlight(documentId, chunkId, documentName)

// å…³é”®è¯æå–
function extractKeywords(text)

// æ–‡æ¡£æ¨¡æ€æ¡†
function showDocumentModal(title, content, isHighlighted, subtitle)
```

**ç”¨æˆ·ä½“éªŒä¼˜åŒ–**
- å“åº”å¼è®¾è®¡é€‚é…ç§»åŠ¨ç«¯
- åŠ è½½çŠ¶æ€å’Œé”™è¯¯æç¤º
- é”®ç›˜å¿«æ·é”®æ”¯æŒ
- è‡ªåŠ¨æ»šåŠ¨åˆ°é«˜äº®ä½ç½®

### æ ·å¼ç‰¹æ€§

**ç°ä»£åŒ–è®¾è®¡**
- æ¸å˜èƒŒæ™¯å’Œé˜´å½±æ•ˆæœ
- å¹³æ»‘åŠ¨ç”»è¿‡æ¸¡
- è‡ªå®šä¹‰æ»šåŠ¨æ¡
- é«˜äº®æ–‡æœ¬åŠ¨ç”»

**äº¤äº’åé¦ˆ**
- æŒ‰é’®æ‚¬åœæ•ˆæœ
- æ¨¡æ€æ¡†æ·¡å…¥åŠ¨ç”»
- é«˜äº®è„‰å†²æ•ˆæœ
- åŠ è½½çŠ¶æ€æŒ‡ç¤º

---

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

```python
# config.py

# æ•°æ®åº“é…ç½®
DATABASE_URL = "postgresql://user:password@localhost/dbname"
REDIS_URL = "redis://localhost:6379"

# OpenAIé…ç½®
OPENAI_API_KEY = "sk-..."
OPENAI_BASE_URL = "https://api.openai.com/v1"

# å‘é‡æ¨¡å‹é…ç½®
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
HF_HOME = "./models"  # HuggingFaceæ¨¡å‹ç¼“å­˜ç›®å½•
HF_OFFLINE = True     # ç¦»çº¿æ¨¡å¼

# ChromaDBé…ç½®
CHROMA_PERSIST_DIRECTORY = "./vector_db/chroma"
CHROMA_COLLECTION = "documents"

# æ–‡ä»¶å­˜å‚¨é…ç½®
STATIC_PATH = "./static"
DOCUMENT_STORE_PATH = "./static/documents"
```

### æ¨¡å‹é…ç½®

**æ¨èçš„å‘é‡æ¨¡å‹**
- `paraphrase-multilingual-MiniLM-L12-v2` - å¤šè¯­è¨€æ”¯æŒï¼Œå¹³è¡¡æ€§èƒ½
- `text2vec-base-chinese` - ä¸­æ–‡ä¼˜åŒ–
- `all-MiniLM-L6-v2` - è‹±æ–‡è½»é‡çº§

**LLMé…ç½®**
- æ¨¡å‹ï¼šGPT-3.5-turbo
- æ¸©åº¦ï¼š0.1ï¼ˆç¡®ä¿å›ç­”ä¸€è‡´æ€§ï¼‰
- æœ€å¤§ä»¤ç‰Œï¼š2000

---

## ğŸš€ éƒ¨ç½²æŒ‡å—

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®åº“åˆå§‹åŒ–

```bash
# æ•°æ®åº“è¿ç§»
aerich init -t config.TORTOISE_ORM
aerich init-db
aerich upgrade

# åˆå§‹åŒ–RAGç³»ç»Ÿ
python init_rag.py
```

### 3. å¯åŠ¨æœåŠ¡

```bash
# å¼€å‘æ¨¡å¼
fastapi dev asgi.py

# ç”Ÿäº§æ¨¡å¼
uvicorn asgi:app --host 0.0.0.0 --port 8000 --workers 4
```

### 4. Dockeréƒ¨ç½²

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "asgi:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5. Nginxé…ç½®

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    client_max_body_size 100M;  # æ”¯æŒå¤§æ–‡ä»¶ä¸Šä¼ 
}
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å‘é‡æœç´¢ä¼˜åŒ–

**ChromaDBä¼˜åŒ–**
```python
# ä½¿ç”¨HNSWç´¢å¼•
collection = client.get_or_create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine", "hnsw:M": 16}
)

# æ‰¹é‡æ“ä½œ
collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)
```

**æœç´¢å‚æ•°è°ƒä¼˜**
- `top_k`: æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´ï¼ˆæ¨è5-10ï¼‰
- `chunk_size`: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ï¼ˆæ¨è800-1200ï¼‰
- `chunk_overlap`: ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼ˆæ¨è15-25%ï¼‰

### 2. æ•°æ®åº“ä¼˜åŒ–

**ç´¢å¼•ä¼˜åŒ–**
```sql
-- æ–‡æ¡£è¡¨ç´¢å¼•
CREATE INDEX idx_document_status ON documents(status);
CREATE INDEX idx_document_upload_time ON documents(upload_time);

-- æ–‡æ¡£å—è¡¨ç´¢å¼•
CREATE INDEX idx_chunk_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunk_index ON document_chunks(chunk_index);
```

**æŸ¥è¯¢ä¼˜åŒ–**
- ä½¿ç”¨å¼‚æ­¥ORMå‡å°‘é˜»å¡
- æ‰¹é‡æŸ¥è¯¢å‡å°‘æ•°æ®åº“è®¿é—®
- é€‚å½“çš„åˆ†é¡µå¤§å°

### 3. ç¼“å­˜ç­–ç•¥

**Redisç¼“å­˜**
```python
# ç¼“å­˜æœç´¢ç»“æœ
@cache(expire=3600)  # 1å°æ—¶ç¼“å­˜
async def search_similar_chunks(query: str, top_k: int):
    # æœç´¢é€»è¾‘
    pass

# ç¼“å­˜æ–‡æ¡£å†…å®¹
@cache(expire=86400)  # 24å°æ—¶ç¼“å­˜
async def get_document_content(document_id: int):
    # è·å–æ–‡æ¡£å†…å®¹
    pass
```

### 4. å¼‚æ­¥å¤„ç†

**åå°ä»»åŠ¡**
```python
from fastapi import BackgroundTasks

@router.post("/upload")
async def upload_document(background_tasks: BackgroundTasks, file: UploadFile):
    # ç«‹å³è¿”å›å“åº”
    document = await create_document_record(file)
    
    # åå°å¤„ç†
    background_tasks.add_task(process_document, document.id)
    
    return {"message": "ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­..."}
```

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜ç—‡çŠ¶**
```
OSError: Can't load tokenizer for 'sentence-transformers/...'
```

**è§£å†³æ–¹æ¡ˆ**
```bash
# è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
export HF_HOME=/path/to/models

# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
python -c "
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
"
```

#### 2. ChromaDBè¿æ¥é”™è¯¯

**é—®é¢˜ç—‡çŠ¶**
```
chromadb.errors.InvalidDimensionException: Embedding dimension mismatch
```

**è§£å†³æ–¹æ¡ˆ**
```python
# æ¸…ç†ChromaDBæ•°æ®
import shutil
shutil.rmtree('./vector_db/chroma')

# é‡æ–°åˆå§‹åŒ–
python init_rag.py
```

#### 3. OpenAI APIè°ƒç”¨å¤±è´¥

**é—®é¢˜ç—‡çŠ¶**
```
openai.error.RateLimitError: Rate limit exceeded
```

**è§£å†³æ–¹æ¡ˆ**
```python
# æ·»åŠ é‡è¯•æœºåˆ¶
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def call_openai_api():
    # APIè°ƒç”¨é€»è¾‘
    pass
```

#### 4. æ–‡æ¡£å¤„ç†è¶…æ—¶

**é—®é¢˜ç—‡çŠ¶**
- å¤§æ–‡ä»¶å¤„ç†æ—¶é—´è¿‡é•¿
- å†…å­˜ä½¿ç”¨è¿‡é«˜

**è§£å†³æ–¹æ¡ˆ**
```python
# åˆ†æ‰¹å¤„ç†å¤§æ–‡æ¡£
async def process_large_document(content: str):
    chunks = split_content_into_batches(content, batch_size=50)
    
    for batch in chunks:
        await process_batch(batch)
        await asyncio.sleep(0.1)  # é¿å…èµ„æºå ç”¨è¿‡é«˜
```

### æ—¥å¿—é…ç½®

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_system.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

### ç›‘æ§æŒ‡æ ‡

**å…³é”®æŒ‡æ ‡**
- æ–‡æ¡£å¤„ç†æˆåŠŸç‡
- å¹³å‡å“åº”æ—¶é—´
- å‘é‡æœç´¢å‡†ç¡®ç‡
- LLMè°ƒç”¨æˆåŠŸç‡
- ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

---

## ğŸ‘¨â€ğŸ’» å¼€å‘æŒ‡å—

### ä»£ç ç»“æ„

```
app/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ api/                 # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ chat/           # èŠå¤©API
â”‚   â”‚   â”œâ”€â”€ documents/      # æ–‡æ¡£API
â”‚   â”‚   â””â”€â”€ users/          # ç”¨æˆ·API
â”‚   â”œâ”€â”€ models/             # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ document.py     # æ–‡æ¡£æ¨¡å‹
â”‚   â”‚   â””â”€â”€ user.py         # ç”¨æˆ·æ¨¡å‹
â”‚   â”œâ”€â”€ utils/              # å·¥å…·ç±»
â”‚   â”‚   â”œâ”€â”€ rag_helper.py   # RAGæ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â””â”€â”€ common.py       # é€šç”¨å·¥å…·
â”‚   â””â”€â”€ dependencies/       # ä¾èµ–æ³¨å…¥
â”œâ”€â”€ static/                 # é™æ€èµ„æº
â”‚   â”œâ”€â”€ chat.html          # èŠå¤©ç•Œé¢
â”‚   â””â”€â”€ upload.html        # ä¸Šä¼ ç•Œé¢
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ asgi.py               # ASGIåº”ç”¨
â””â”€â”€ init_rag.py           # åˆå§‹åŒ–è„šæœ¬
```

### å¼€å‘è§„èŒƒ

**ä»£ç é£æ ¼**
- ä½¿ç”¨Blackæ ¼å¼åŒ–ä»£ç 
- éµå¾ªPEP 8è§„èŒƒ
- æ·»åŠ ç±»å‹æ³¨è§£
- ç¼–å†™è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²

**é”™è¯¯å¤„ç†**
```python
try:
    result = await some_operation()
    logger.info(f"æ“ä½œæˆåŠŸ: {result}")
    return result
except SpecificException as e:
    logger.error(f"ç‰¹å®šé”™è¯¯: {e}")
    raise HTTPException(status_code=400, detail=str(e))
except Exception as e:
    logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
    raise HTTPException(status_code=500, detail="å†…éƒ¨æœåŠ¡å™¨é”™è¯¯")
```

**æµ‹è¯•ç¼–å†™**
```python
import pytest
from fastapi.testclient import TestClient

@pytest.mark.asyncio
async def test_document_upload():
    with TestClient(app) as client:
        with open("test.pdf", "rb") as f:
            response = client.post(
                "/api/v1/documents/upload",
                files={"file": ("test.pdf", f, "application/pdf")}
            )
        assert response.status_code == 200
        assert "ä¸Šä¼ æˆåŠŸ" in response.json()["message"]
```

### æ‰©å±•å¼€å‘

**æ·»åŠ æ–°çš„æ–‡æ¡£æ ¼å¼æ”¯æŒ**
```python
class DocumentProcessor:
    async def _extract_content(self, file_path: str, file_type: str) -> str:
        if file_type == "new_format":
            return await self._extract_new_format_content(file_path)
        # ç°æœ‰é€»è¾‘...
    
    async def _extract_new_format_content(self, file_path: str) -> str:
        # æ–°æ ¼å¼å¤„ç†é€»è¾‘
        pass
```

**è‡ªå®šä¹‰å‘é‡æ¨¡å‹**
```python
from sentence_transformers import SentenceTransformer

class CustomEmbeddingModel:
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)
    
    def encode(self, texts: List[str]) -> np.ndarray:
        # è‡ªå®šä¹‰ç¼–ç é€»è¾‘
        return self.model.encode(texts)
```

---

## ğŸ“Š ç³»ç»Ÿç›‘æ§

### æ€§èƒ½æŒ‡æ ‡

**å“åº”æ—¶é—´ç›‘æ§**
```python
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        end_time = time.time()
        
        logger.info(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
        return result
    return wrapper

@monitor_performance
async def search_similar_chunks(query: str, top_k: int):
    # æœç´¢é€»è¾‘
    pass
```

**èµ„æºä½¿ç”¨ç›‘æ§**
```python
import psutil

def log_system_stats():
    cpu_percent = psutil.cpu_percent()
    memory_percent = psutil.virtual_memory().percent
    disk_percent = psutil.disk_usage('/').percent
    
    logger.info(f"ç³»ç»Ÿèµ„æºä½¿ç”¨ - CPU: {cpu_percent}%, å†…å­˜: {memory_percent}%, ç£ç›˜: {disk_percent}%")
```

### å¥åº·æ£€æŸ¥

```python
@router.get("/health")
async def health_check():
    checks = {
        "database": await check_database_connection(),
        "vector_db": await check_vector_db_connection(),
        "llm": await check_llm_availability(),
        "storage": check_storage_space()
    }
    
    all_healthy = all(checks.values())
    status_code = 200 if all_healthy else 503
    
    return JSONResponse(
        status_code=status_code,
        content={"status": "healthy" if all_healthy else "unhealthy", "checks": checks}
    )
```

---

## ğŸ”® æœªæ¥è§„åˆ’

### åŠŸèƒ½æ‰©å±•

1. **å¤šæ¨¡æ€æ”¯æŒ** - å›¾ç‰‡ã€éŸ³é¢‘æ–‡æ¡£å¤„ç†
2. **å®æ—¶åä½œ** - å¤šç”¨æˆ·åŒæ—¶ç¼–è¾‘å’Œé—®ç­”
3. **çŸ¥è¯†å›¾è°±** - æ–‡æ¡£é—´å…³ç³»å»ºæ¨¡
4. **ä¸ªæ€§åŒ–æ¨è** - åŸºäºç”¨æˆ·å†å²çš„æ™ºèƒ½æ¨è
5. **APIé›†æˆ** - æ”¯æŒæ›´å¤šç¬¬ä¸‰æ–¹æœåŠ¡

### æŠ€æœ¯ä¼˜åŒ–

1. **åˆ†å¸ƒå¼éƒ¨ç½²** - æ”¯æŒé›†ç¾¤éƒ¨ç½²å’Œè´Ÿè½½å‡è¡¡
2. **æµå¼å¤„ç†** - å®æ—¶æ–‡æ¡£å¤„ç†å’Œå¢é‡æ›´æ–°
3. **æ¨¡å‹ä¼˜åŒ–** - è‡ªå®šä¹‰è®­ç»ƒå’Œæ¨¡å‹å‹ç¼©
4. **ç¼“å­˜ä¼˜åŒ–** - å¤šçº§ç¼“å­˜å’Œæ™ºèƒ½é¢„åŠ è½½
5. **å®‰å…¨å¢å¼º** - æ•°æ®åŠ å¯†å’Œè®¿é—®æ§åˆ¶

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è”ç³»æ–¹å¼

- **é¡¹ç›®ä»“åº“**: [GitHubé“¾æ¥]
- **æŠ€æœ¯æ–‡æ¡£**: [æ–‡æ¡£é“¾æ¥]
- **é—®é¢˜åé¦ˆ**: [Issueé“¾æ¥]

### è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Pull Requestå’ŒIssueï¼è¯·ç¡®ä¿ï¼š

1. ä»£ç ç¬¦åˆé¡¹ç›®è§„èŒƒ
2. æ·»åŠ é€‚å½“çš„æµ‹è¯•
3. æ›´æ–°ç›¸å…³æ–‡æ¡£
4. è¯¦ç»†æè¿°å˜æ›´å†…å®¹

---

*æœ€åæ›´æ–°æ—¶é—´: 2024å¹´10æœˆ*
*æ–‡æ¡£ç‰ˆæœ¬: v1.0*
