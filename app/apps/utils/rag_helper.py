"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—

è¯¥æ¨¡å—åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
1. DocumentProcessor: æ–‡æ¡£å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–
2. VectorSearch: å‘é‡æœç´¢å¼•æ“ï¼ŒåŸºäºChromaæ•°æ®åº“è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
3. RAGGenerator: RAGç”Ÿæˆå™¨ï¼Œé›†æˆLangChainå’ŒOpenAIè¿›è¡Œæ™ºèƒ½é—®ç­”

æŠ€æœ¯æ ˆï¼š
- æ–‡æ¡£è§£æ: ç‹¬ç«‹çš„DocumentParseræ¨¡å—
- æ–‡æœ¬åˆ†å‰²: LangChain RecursiveCharacterTextSplitter
- å‘é‡åŒ–: Sentence Transformers
- å‘é‡å­˜å‚¨: ChromaDB
- æ™ºèƒ½é—®ç­”: LangChain + OpenAI GPT

æ¶æ„è¯´æ˜ï¼š
- æ–‡æ¡£è§£æåŠŸèƒ½å·²æ‹†åˆ†åˆ°ç‹¬ç«‹çš„document_parseræ¨¡å—
- æœ¬æ¨¡å—ä¸“æ³¨äºå‘é‡åŒ–ã€æœç´¢å’Œç”ŸæˆåŠŸèƒ½
- é€šè¿‡ä¾èµ–æ³¨å…¥çš„æ–¹å¼ä½¿ç”¨æ–‡æ¡£è§£æå™¨
"""

import os
import traceback
import numpy as np
from typing import List, Dict, Any

import chromadb
from chromadb.config import Settings as ChromaSettings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer

from apps.models.document import DocumentChunk
from apps.utils.common import get_local_model_path
from config import (
    CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION, EMBEDDING_MODEL,
    HF_HOME, HF_OFFLINE, OPENAI_API_KEY, OPENAI_BASE_URL, SIMILARITY_THRESHOLD
)


class VectorSearch:
    """
    å‘é‡æœç´¢å¼•æ“ - åŸºäºChromaDBçš„è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    2. å‘é‡æ•°æ®ç®¡ç†
    3. æœç´¢ç»“æœæ’åºå’Œè¿‡æ»¤
    4. ä¸æ•°æ®åº“æ•°æ®å…³è”
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        
        é…ç½®ç»„ä»¶ï¼š
        1. å‘é‡åµŒå…¥æ¨¡å‹ï¼ˆä¸DocumentProcessorä¿æŒä¸€è‡´ï¼‰
        2. ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ
        """
        try:
            # é…ç½®HuggingFaceç¯å¢ƒ
            os.environ["HF_HOME"] = HF_HOME
            os.environ["TRANSFORMERS_CACHE"] = HF_HOME
            os.environ["HF_HUB_CACHE"] = HF_HOME
            
            if HF_OFFLINE:
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"
            
            # å…³é—­Chromaé¥æµ‹
            os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
            os.environ.setdefault("CHROMA_TELEMETRY", "false")
            
            # åˆå§‹åŒ–å‘é‡åµŒå…¥æ¨¡å‹ï¼ˆä¸DocumentProcessorä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼‰
            local_model_path = get_local_model_path(EMBEDDING_MODEL, HF_HOME)
            
            if local_model_path and HF_OFFLINE:
                self.embedding_model = SentenceTransformer(local_model_path)
            else:
                self.embedding_model = SentenceTransformer(
                    EMBEDDING_MODEL, 
                    cache_folder=HF_HOME
                )
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(
                path=CHROMA_PERSIST_DIRECTORY, 
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            self.collection = self.chroma_client.get_or_create_collection(
                name=CHROMA_COLLECTION, 
                metadata={"hnsw:space": "cosine"}
            )
            
        except Exception as e:
            raise Exception(f"VectorSearchåˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def search_similar_chunks(self, query: str, top_k: int = 5, use_threshold: bool = True) -> List[Dict[str, Any]]:
        """
        æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£å—
        
        æœç´¢æµç¨‹ï¼š
        1. å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        2. åœ¨ChromaDBä¸­è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
        3. ä»æ•°æ®åº“è·å–å®Œæ•´çš„æ–‡æ¡£å’Œå—ä¿¡æ¯
        4. è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°å¹¶æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            
        Returns:
            List[Dict]: ç›¸ä¼¼æ–‡æ¡£å—åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æ¡£ã€å—å’Œç›¸ä¼¼åº¦ä¿¡æ¯
        """
        try:
            if not query or not query.strip():
                return []
            
            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode([query.strip()])[0]
            
            # 2. åœ¨ChromaDBä¸­æœç´¢ç›¸ä¼¼å‘é‡
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()], 
                n_results=min(top_k, 20)  # é™åˆ¶æœ€å¤§æœç´¢æ•°é‡
            )
            
            # 3. è§£ææœç´¢ç»“æœ
            ids = results.get('ids', [[]])[0]
            metadatas = results.get('metadatas', [[]])[0]
            distances = results.get('distances', [[]])[0] or []
            
            # 4. ä»æ•°æ®åº“è·å–å®Œæ•´ä¿¡æ¯å¹¶æ„å»ºç»“æœ
            all_similarities = []  # å­˜å‚¨æ‰€æœ‰ç»“æœ
            filtered_similarities = []  # å­˜å‚¨è¿‡æ»¤åçš„ç»“æœ
            
            for i, (vector_id, metadata) in enumerate(zip(ids, metadatas)):
                try:
                    # è·å–æ–‡æ¡£å—ä¿¡æ¯
                    chunk_id = metadata.get('chunk_id')
                    if not chunk_id:
                        continue
                    
                    chunk = await DocumentChunk.get_or_none(id=chunk_id)
                    if not chunk:
                        continue
                    
                    # è·å–å…³è”çš„æ–‡æ¡£
                    document = await chunk.document
                    if not document:
                        continue
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
                    distance = distances[i] if i < len(distances) else 1.0
                    similarity = max(0.0, 1.0 - float(distance))
                    
                    result_item = {
                        'vector_id': vector_id,
                        'similarity': similarity,
                        'chunk': chunk,
                        'document': document,
                        'metadata': metadata,
                        'above_threshold': similarity >= SIMILARITY_THRESHOLD
                    }
                    
                    all_similarities.append(result_item)
                    
                    # å¦‚æœä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåªä¿ç•™ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ç»“æœ
                    if not use_threshold or similarity >= SIMILARITY_THRESHOLD:
                        filtered_similarities.append(result_item)
                    
                except Exception as e:
                    print(f"å¤„ç†æœç´¢ç»“æœé¡¹å¤±è´¥ (chunk_id: {metadata.get('chunk_id', 'unknown')}): {e}")
                    continue
            
            # 5. æ™ºèƒ½é€‰æ‹©è¿”å›ç»“æœ
            if use_threshold and filtered_similarities:
                # æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œè¿”å›è¿‡æ»¤åçš„ç»“æœ
                filtered_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                return filtered_similarities[:top_k]
            elif use_threshold and not filtered_similarities and all_similarities:
                # æ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œä½†æœ‰æœç´¢ç»“æœï¼Œè¿”å›æœ€ç›¸ä¼¼çš„å‡ ä¸ªå¹¶æ ‡è®°
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                # å–å‰å‡ ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœï¼Œä½†æ ‡è®°ä¸ºä½ç›¸ä¼¼åº¦
                return all_similarities[:min(top_k, 3)]  # æœ€å¤šè¿”å›3ä¸ªä½ç›¸ä¼¼åº¦ç»“æœ
            else:
                # ä¸ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œè¿”å›æ‰€æœ‰ç»“æœ
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                return all_similarities[:top_k]
            
        except Exception as e:
            print(f"æœç´¢ç›¸ä¼¼æ–‡æ¡£å—å¤±è´¥: {e}")
            traceback.print_exc()
            return []

    async def delete_document_vectors(self, document_id: int):
        """
        åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡æ•°æ®
        
        Args:
            document_id: æ–‡æ¡£ID
        """
        try:
            # é€šè¿‡metadataæ¡ä»¶åˆ é™¤
            self.collection.delete(where={"document_id": document_id})
            
        except Exception as e:
            raise Exception(f"åˆ é™¤æ–‡æ¡£ {document_id} å‘é‡æ•°æ®å¤±è´¥: {e}")

    async def count_vectors(self) -> int:
        """
        ç»Ÿè®¡å‘é‡æ•°æ®åº“ä¸­çš„å‘é‡æ€»æ•°
        
        Returns:
            int: å‘é‡æ€»æ•°
        """
        try:
            count = self.collection.count()
            return int(count) if isinstance(count, (int, float)) else 0
            
        except Exception:
            return 0

    def apply_mmr(self, results: List[Dict[str, Any]], query_embedding: np.ndarray, 
                  lambda_param: float = 0.7, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        åº”ç”¨æœ€å¤§è¾¹ç•Œç®—æ³•(MMR)æ¥ä¼˜åŒ–æœç´¢ç»“æœ
        
        MMRç®—æ³•åœ¨ä¿æŒç›¸å…³æ€§çš„åŒæ—¶å¢åŠ ç»“æœçš„å¤šæ ·æ€§ï¼Œé¿å…è¿”å›è¿‡äºç›¸ä¼¼çš„æ–‡æ¡£ç‰‡æ®µã€‚
        
        Args:
            results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
            query_embedding: æŸ¥è¯¢å‘é‡
            lambda_param: MMRå‚æ•°ï¼Œæ§åˆ¶ç›¸å…³æ€§å’Œå¤šæ ·æ€§çš„å¹³è¡¡ (0-1)
                         0.0: åªè€ƒè™‘å¤šæ ·æ€§
                         1.0: åªè€ƒè™‘ç›¸å…³æ€§
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: ç»è¿‡MMRä¼˜åŒ–çš„ç»“æœåˆ—è¡¨
        """
        try:
            if not results or len(results) <= 1:
                return results[:top_k]
            
            # æå–æ‰€æœ‰æ–‡æ¡£çš„å‘é‡
            doc_embeddings = []
            for result in results:
                # ä»metadataä¸­è·å–å‘é‡IDï¼Œç„¶åä»ChromaDBè·å–å‘é‡
                vector_id = result.get('vector_id')
                if vector_id:
                    try:
                        # ä»ChromaDBè·å–å‘é‡
                        vector_data = self.collection.get(ids=[vector_id])
                        if vector_data and 'embeddings' in vector_data and vector_data['embeddings']:
                            doc_embeddings.append(np.array(vector_data['embeddings'][0]))
                        else:
                            # å¦‚æœæ— æ³•è·å–å‘é‡ï¼Œä½¿ç”¨é›¶å‘é‡
                            doc_embeddings.append(np.zeros_like(query_embedding))
                    except:
                        doc_embeddings.append(np.zeros_like(query_embedding))
                else:
                    doc_embeddings.append(np.zeros_like(query_embedding))
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            doc_embeddings = np.array(doc_embeddings)
            
            # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
            query_similarities = np.dot(doc_embeddings, query_embedding)
            
            # MMRç®—æ³•
            selected_indices = []
            remaining_indices = list(range(len(results)))
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
            first_idx = np.argmax(query_similarities)
            selected_indices.append(first_idx)
            remaining_indices.remove(first_idx)
            
            # è¿­ä»£é€‰æ‹©å‰©ä½™æ–‡æ¡£
            while len(selected_indices) < min(top_k, len(results)) and remaining_indices:
                best_score = -float('inf')
                best_idx = None
                
                for idx in remaining_indices:
                    # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                    relevance = query_similarities[idx]
                    
                    # è®¡ç®—å¤šæ ·æ€§åˆ†æ•°ï¼ˆä¸å·²é€‰æ‹©æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼‰
                    max_similarity = 0
                    if selected_indices:
                        selected_embeddings = doc_embeddings[selected_indices]
                        similarities = np.dot(doc_embeddings[idx], selected_embeddings.T)
                        max_similarity = np.max(similarities)
                    
                    # MMRåˆ†æ•° = Î» * ç›¸å…³æ€§ - (1-Î») * å¤šæ ·æ€§
                    mmr_score = lambda_param * relevance - (1 - lambda_param) * max_similarity
                    
                    if mmr_score > best_score:
                        best_score = mmr_score
                        best_idx = idx
                
                if best_idx is not None:
                    selected_indices.append(best_idx)
                    remaining_indices.remove(best_idx)
                else:
                    break
            
            # è¿”å›æŒ‰MMRç®—æ³•é€‰æ‹©çš„ç»“æœ
            mmr_results = [results[i] for i in selected_indices]
            return mmr_results
            
        except Exception as e:
            print(f"MMRç®—æ³•åº”ç”¨å¤±è´¥: {e}")
            # å¦‚æœMMRå¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ
            return results[:top_k]

    async def search_similar_chunks_with_mmr(self, query: str, top_k: int = 5, 
                                           use_threshold: bool = True, 
                                           lambda_param: float = 0.7) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨MMRç®—æ³•æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£å—
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            lambda_param: MMRå‚æ•°ï¼Œæ§åˆ¶ç›¸å…³æ€§å’Œå¤šæ ·æ€§çš„å¹³è¡¡
            
        Returns:
            List[Dict]: ç»è¿‡MMRä¼˜åŒ–çš„ç›¸ä¼¼æ–‡æ¡£å—åˆ—è¡¨
        """
        try:
            if not query or not query.strip():
                return []
            
            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode([query.strip()])[0]
            
            # 2. åœ¨ChromaDBä¸­æœç´¢æ›´å¤šç›¸ä¼¼å‘é‡ï¼ˆç”¨äºMMRç®—æ³•ï¼‰
            search_k = min(top_k * 3, 20)  # æœç´¢æ›´å¤šç»“æœç”¨äºMMRé€‰æ‹©
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()], 
                n_results=search_k
            )
            
            if not results or not results.get('ids') or not results['ids'][0]:
                return []
            
            # 3. å¤„ç†æœç´¢ç»“æœ
            all_similarities = []
            filtered_similarities = []
            
            ids = results['ids'][0]
            distances = results['distances'][0] if results.get('distances') else []
            metadatas = results['metadatas'][0] if results.get('metadatas') else []
            
            for i, (vector_id, metadata) in enumerate(zip(ids, metadatas)):
                try:
                    # è·å–æ–‡æ¡£å—ä¿¡æ¯
                    chunk_id = metadata.get('chunk_id')
                    if not chunk_id:
                        continue
                    
                    chunk = await DocumentChunk.get_or_none(id=chunk_id)
                    if not chunk:
                        continue
                    
                    # è·å–å…³è”çš„æ–‡æ¡£
                    document = await chunk.document
                    if not document:
                        continue
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                    distance = distances[i] if i < len(distances) else 1.0
                    similarity = max(0.0, 1.0 - float(distance))
                    
                    result_item = {
                        'vector_id': vector_id,
                        'similarity': similarity,
                        'chunk': chunk,
                        'document': document,
                        'metadata': metadata,
                        'above_threshold': similarity >= SIMILARITY_THRESHOLD
                    }
                    
                    all_similarities.append(result_item)
                    
                    # å¦‚æœä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåªä¿ç•™ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ç»“æœ
                    if not use_threshold or similarity >= SIMILARITY_THRESHOLD:
                        filtered_similarities.append(result_item)
                    
                except Exception as e:
                    print(f"å¤„ç†æœç´¢ç»“æœé¡¹å¤±è´¥ (chunk_id: {metadata.get('chunk_id', 'unknown')}): {e}")
                    continue
            
            # 4. åº”ç”¨MMRç®—æ³•
            if filtered_similarities:
                # æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œå¯¹è¿‡æ»¤åçš„ç»“æœåº”ç”¨MMR
                mmr_results = self.apply_mmr(filtered_similarities, query_embedding, lambda_param, top_k)
                return mmr_results
            elif all_similarities:
                # æ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œå¯¹æ‰€æœ‰ç»“æœåº”ç”¨MMR
                mmr_results = self.apply_mmr(all_similarities, query_embedding, lambda_param, min(top_k, 3))
                return mmr_results
            else:
                return []
            
        except Exception as e:
            print(f"MMRæœç´¢ç›¸ä¼¼æ–‡æ¡£å—å¤±è´¥: {e}")
            traceback.print_exc()
            return []

class RAGGenerator:
    """
    RAGç”Ÿæˆå™¨ - æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. é›†æˆå‘é‡æœç´¢å’ŒLLMç”Ÿæˆ
    2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
    3. æ”¯æŒå¤šç§å›ç­”æ¨¡å¼ï¼ˆLLMæ™ºèƒ½å›ç­” / ç®€å•å›ç­”ï¼‰
    4. æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—®ç­”ä½“éªŒ
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–RAGç”Ÿæˆå™¨
        
        ç»„ä»¶ï¼š
        1. VectorSearch: å‘é‡æœç´¢å¼•æ“
        2. LangChain + OpenAI: æ™ºèƒ½é—®ç­”é“¾ï¼ˆå¯é€‰ï¼‰
        3. å¤‡ç”¨ç®€å•å›ç­”æ¨¡å¼
        """
        try:
            # åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
            self.vector_search = VectorSearch()
            
            # åˆå§‹åŒ–LLMç»„ä»¶
            self.llm = None
            self.chain = None
            self.llm_available = False
            
            # æ£€æŸ¥OpenAIé…ç½®å¹¶åˆå§‹åŒ–LLM
            if OPENAI_API_KEY and OPENAI_API_KEY.strip():
                try:
                    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
                    self.llm = ChatOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿å›ç­”çš„ä¸€è‡´æ€§
                        model="gpt-3.5-turbo",
                        max_tokens=2000  # é™åˆ¶å›ç­”é•¿åº¦
                    )
                    
                    # åˆ›å»ºç³»ç»Ÿæç¤ºæ¨¡æ¿
                    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„ï¼š
   - ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜ç»„ç»‡å†…å®¹
   - é‡è¦ä¿¡æ¯ç”¨**ç²—ä½“**æ ‡è®°
   - ä½¿ç”¨é¡¹ç›®ç¬¦å·(â€¢)æˆ–æ•°å­—åˆ—è¡¨å±•ç¤ºè¦ç‚¹
   - æ¯ä¸ªæ®µè½ä¸“æ³¨ä¸€ä¸ªä¸»é¢˜ï¼Œæ®µè½é—´ç•™ç©ºè¡Œ
   - å¤æ‚å†…å®¹ä½¿ç”¨è¡¨æ ¼æˆ–ç»“æ„åŒ–æ ¼å¼
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œå…³é”®å†…å®¹ç‰‡æ®µï¼Œæ ¼å¼ï¼šã€Œæ–‡æ¡£åï¼šå…·ä½“å†…å®¹ã€
5. å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£æä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶æ ‡æ˜ä¿¡æ¯æ¥æº
6. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚å½“ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}"""

                    human_template = "é—®é¢˜ï¼š{question}"
                    
                    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
                    chat_prompt = ChatPromptTemplate([
                        ("system", system_template),
                        ("human", human_template),
                    ])
                    
                    # åˆ›å»ºè¾“å‡ºè§£æå™¨
                    output_parser = StrOutputParser()
                    
                    # åˆ›å»ºLangChainå¤„ç†é“¾
                    self.chain = chat_prompt | self.llm | output_parser
                    self.llm_available = True
                    
                except Exception:
                    self.llm = None
                    self.chain = None
                    self.llm_available = False
            else:
                self.llm_available = False
                
        except Exception as e:
            raise Exception(f"RAGGeneratoråˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        å›ç­”ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨LLMæ™ºèƒ½å›ç­”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. é™çº§åˆ°ç®€å•å›ç­”æ¨¡å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        3. æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ¥æºå¼•ç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                content = chunk['chunk'].content
                similarity = chunk['similarity']
                
                # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
                context_part = f"""æ–‡æ¡£ {i}: {doc_name}
ç›¸ä¼¼åº¦: {similarity:.2f}
å†…å®¹: {content}"""
                context_parts.append(context_part)
            
            context = "\n\n" + "="*50 + "\n\n".join(context_parts)
            
            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    answer = await self._llm_answer(query, context)
                    
                    # ä¸å†è‡ªåŠ¨æ·»åŠ æ¥æºä¿¡æ¯ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦æ˜¾ç¤º
                    return answer
                    
                except Exception:
                    return self._simple_answer(query, context_chunks)
            else:
                return self._simple_answer(query, context_chunks)
                
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    async def generate_answer_stream(self, query: str, context_chunks: List[Dict[str, Any]]):
        """
        æµå¼ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ–‡æ¡£ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk.get('document_name', 'æœªçŸ¥æ–‡æ¡£')
                # å°è¯•è·å–chunk_textå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨contentå­—æ®µ
                content = chunk.get('chunk_text', chunk.get('content', ''))
                context_parts.append(f"æ–‡æ¡£{i}: {doc_name}\nå†…å®¹: {content}")
            
            context = "\n\n" + "="*50 + "\n\n".join(context_parts)
            
            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    async for chunk in self._llm_answer_stream(query, context):
                        yield chunk
                except Exception as e:
                    print(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {e}")
                    # é™çº§åˆ°ç®€å•å›ç­”
                    simple_answer = self._simple_answer(query, context_chunks)
                    yield simple_answer
            else:
                # éLLMæ¨¡å¼ï¼Œç›´æ¥è¿”å›ç®€å•å›ç­”
                simple_answer = self._simple_answer(query, context_chunks)
                yield simple_answer
                
        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def _llm_answer_stream(self, query: str, context: str):
        """
        ä½¿ç”¨LangChainé“¾å¼æµå¼ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„chainè¿›è¡Œæµå¼è°ƒç”¨
            if self.chain:
                async for chunk in self.chain.astream({
                    "question": query,
                    "context": context
                }):
                    if chunk:
                        yield chunk
            else:
                raise Exception("LangChainå¤„ç†é“¾æœªåˆå§‹åŒ–")
                    
        except Exception as e:
            traceback.print_exc()
            raise Exception(f"LLMé“¾å¼æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
    
    async def _llm_answer(self, query: str, context: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Returns:
            str: LLMç”Ÿæˆçš„å›ç­”
        """
        try:
            # è°ƒç”¨LangChainå¤„ç†é“¾
            response = await self.chain.ainvoke({
                "question": query,
                "context": context
            })
            
            if not response or not response.strip():
                raise Exception("LLMè¿”å›ç©ºå›ç­”")
            
            return response.strip()
            
        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        ç®€å•å›ç­”æ¨¡å¼ - åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç®€å•æ¨¡å¼ç”Ÿæˆçš„å›ç­”
        """
        try:
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            
            # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
            best_chunk = context_chunks[0]
            document_name = best_chunk['document'].original_filename or best_chunk['document'].filename
            content = best_chunk['chunk'].content
            similarity = best_chunk['similarity']
            
            # æ„å»ºç®€å•å›ç­”ï¼ˆä¸åŒ…å«å‚è€ƒæ¥æºï¼‰
            answer = f"""åŸºäºæ–‡æ¡£ã€Š{document_name}ã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼š

{content}

ğŸ’¡ **æç¤º**ï¼šå½“å‰ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼ã€‚é…ç½®OpenAI API Keyåå¯è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"""
            
            return answer
            
        except Exception:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def _build_sources_info(self, context_chunks: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºæ¥æºä¿¡æ¯
        
        Args:
            context_chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ¥æºä¿¡æ¯
        """
        try:
            sources = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                similarity = chunk['similarity']
                sources.append(f"â€¢ {doc_name} (ç›¸ä¼¼åº¦: {similarity:.1%})")
            
            sources_text = "\n".join(sources)
            return f"""---
ğŸ“‹ **å‚è€ƒæ¥æº**ï¼š
{sources_text}

ğŸ’¡ åŸºäº {len(context_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µç”Ÿæˆæ­¤å›ç­”"""
            
        except Exception:
            return "---\nğŸ“‹ **å‚è€ƒæ¥æº**ï¼šä¿¡æ¯è·å–å¤±è´¥"


# å…¨å±€å®ä¾‹ - å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿æ•´ä¸ªåº”ç”¨ä½¿ç”¨ç›¸åŒçš„å®ä¾‹
try:
    # å‘é‡æœç´¢å®ä¾‹
    vector_search = VectorSearch()
    
    # RAGç”Ÿæˆå™¨å®ä¾‹
    rag_generator = RAGGenerator()
    
except Exception as e:
    raise Exception(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
