import os
import uuid
from typing import List, Dict, Any

import chromadb
import openpyxl
import pypdf
from chromadb.config import Settings as ChromaSettings
from docx import Document as DocxDocument
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import SentenceTransformer

from apps.models.document import Document as DocumentModel, DocumentChunk
from config import CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION, EMBEDDING_MODEL, HF_HOME, HF_OFFLINE, OPENAI_API_KEY, OPENAI_BASE_URL


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        os.environ.setdefault("HF_HOME", HF_HOME)
        if HF_OFFLINE:
            os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
            os.environ.setdefault("HF_HUB_OFFLINE", "1")
        # å…³é—­ Chroma é¥æµ‹ï¼ˆé¿å… ClientCreateCollectionEvent æŠ¥é”™ï¼‰
        os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
        os.environ.setdefault("CHROMA_TELEMETRY", "false")
        # ä¼˜å…ˆæœ¬åœ°ç¼“å­˜çš„ HuggingFace æ¨¡å‹
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL, cache_folder=HF_HOME)
        # åˆå§‹åŒ– Chroma å®¢æˆ·ç«¯
        os.makedirs(CHROMA_PERSIST_DIRECTORY, exist_ok=True)
        self.chroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIRECTORY, settings=ChromaSettings(anonymized_telemetry=False))
        self.collection = self.chroma_client.get_or_create_collection(name=CHROMA_COLLECTION, metadata={"hnsw:space": "cosine"})
        
    async def process_document(self, document_id: int, file_path: str, file_type: str) -> bool:
        """å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡"""
        try:
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤„ç†ä¸­
            document = await DocumentModel.get(id=document_id)
            document.status = "processing"
            await document.save()
            
            # æå–æ–‡æ¡£å†…å®¹
            content = await self._extract_content(file_path, file_type)
            if not content:
                raise Exception("æ— æ³•æå–æ–‡æ¡£å†…å®¹")
            
            # æ›´æ–°æ–‡æ¡£å†…å®¹
            document.content = content
            await document.save()
            
            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_text(content)
            
            # åˆ›å»ºåˆ†å—è®°å½•
            chunk_objects = []
            for i, chunk_text in enumerate(chunks):
                chunk = await DocumentChunk.create(
                    document_id=document_id,
                    chunk_index=i,
                    content=chunk_text,
                    content_length=len(chunk_text),
                    metadata={"chunk_index": i}
                )
                chunk_objects.append(chunk)
            
            # ç”Ÿæˆå‘é‡åµŒå…¥
            embeddings = self.embedding_model.encode(chunks)
            
            # å­˜å‚¨å‘é‡è‡³ Chromaï¼ˆä½œä¸ºå”¯ä¸€äº‹å®æ¥æºï¼‰
            ids = []
            metadatas = []
            vectors = []
            for i, (chunk, embedding) in enumerate(zip(chunk_objects, embeddings)):
                vector_id = f"doc_{document_id}_chunk_{i}_{uuid.uuid4().hex[:8]}"
                ids.append(vector_id)
                metadatas.append({
                    "document_id": document_id,
                    "chunk_id": chunk.id,
                    "chunk_index": i,
                })
                vectors.append(embedding.tolist())
            if ids:
                self.collection.add(ids=ids, embeddings=vectors, metadatas=metadatas)
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå®Œæˆ
            document.status = "completed"
            document.process_time = document.process_time
            await document.save()
            
            return True
            
        except Exception as e:
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
            document = await DocumentModel.get(id=document_id)
            document.status = "failed"
            document.error_message = str(e)
            await document.save()
            return False
    
    async def _extract_content(self, file_path: str, file_type: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹æå–å†…å®¹"""
        try:
            if file_type == "pdf":
                return await self._extract_pdf_content(file_path)
            elif file_type in ["docx", "doc"]:
                return await self._extract_docx_content(file_path)
            elif file_type in ["xlsx", "xls"]:
                return await self._extract_excel_content(file_path)
            elif file_type == "txt":
                return await self._extract_txt_content(file_path)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
        except Exception as e:
            raise Exception(f"æå–æ–‡æ¡£å†…å®¹å¤±è´¥: {str(e)}")
    
    async def _extract_pdf_content(self, file_path: str) -> str:
        """æå–PDFå†…å®¹"""
        content = ""
        with open(file_path, 'rb') as file:
            pdf_reader = pypdf.PdfReader(file)
            for page in pdf_reader.pages:
                content += page.extract_text() + "\n"
        return content
    
    async def _extract_docx_content(self, file_path: str) -> str:
        """æå–DOCXå†…å®¹"""
        doc = DocxDocument(file_path)
        content = ""
        for paragraph in doc.paragraphs:
            content += paragraph.text + "\n"
        return content
    
    async def _extract_excel_content(self, file_path: str) -> str:
        """æå–Excelå†…å®¹"""
        workbook = openpyxl.load_workbook(file_path)
        content = ""
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]
            content += f"å·¥ä½œè¡¨: {sheet_name}\n"
            for row in sheet.iter_rows(values_only=True):
                row_text = "\t".join([str(cell) if cell is not None else "" for cell in row])
                content += row_text + "\n"
        return content
    
    async def _extract_txt_content(self, file_path: str) -> str:
        """æå–TXTå†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()


class VectorSearch:
    """å‘é‡æœç´¢ï¼ˆChroma ä¼˜å…ˆï¼‰"""
    
    def __init__(self):
        os.environ.setdefault("HF_HOME", HF_HOME)
        if HF_OFFLINE:
            os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
            os.environ.setdefault("HF_HUB_OFFLINE", "1")
        # å…³é—­ Chroma é¥æµ‹ï¼ˆé¿å… ClientCreateCollectionEvent æŠ¥é”™ï¼‰
        os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
        os.environ.setdefault("CHROMA_TELEMETRY", "false")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL, cache_folder=HF_HOME)
        self.chroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIRECTORY, settings=ChromaSettings(anonymized_telemetry=False))
        self.collection = self.chroma_client.get_or_create_collection(name=CHROMA_COLLECTION, metadata={"hnsw:space": "cosine"})
    
    async def search_similar_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£å—"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode([query])[0]
        
        # ä¼˜å…ˆä» Chroma æ£€ç´¢
        results = self.collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_k)
        similarities = []
        ids = results.get('ids', [[]])[0]
        metadatas = results.get('metadatas', [[]])[0]
        distances = results.get('distances', [[]])[0] or []
        for idx, meta in zip(ids, metadatas):
            # ä»æ•°æ®åº“è¡¥é½ chunk ä¸ document ä¿¡æ¯
            chunk = await DocumentChunk.get_or_none(id=meta.get('chunk_id'))
            if not chunk:
                continue
            # è·å–å…³è”çš„æ–‡æ¡£
            document = await chunk.document
            similarity = 1 - float(distances[ids.index(idx)]) if distances else 0.0
            similarities.append({
                'vector': None,
                'similarity': similarity,
                'chunk': chunk,
                'document': document
            })
        return similarities

    async def delete_document_vectors(self, document_id: int):
        """åˆ é™¤æŸæ–‡æ¡£åœ¨ Chroma ä¸­çš„æ‰€æœ‰å‘é‡"""
        # é€šè¿‡ metadata åˆ é™¤
        self.collection.delete(where={"document_id": document_id})

    async def count_vectors(self) -> int:
        """ç»Ÿè®¡å‘é‡æ€»é‡ï¼ˆChroma é›†åˆï¼‰"""
        try:
            info = self.collection.count()
            return int(info) if isinstance(info, (int, float)) else 0
        except Exception:
            return 0


class RAGGenerator:
    """RAGç”Ÿæˆå™¨ - é›†æˆLangChainå’ŒOpenAI"""
    
    def __init__(self):
        self.vector_search = VectorSearch()
        
        # åˆå§‹åŒ–LangChainç»„ä»¶
        self.llm = None
        self.chain = None
        
        # å¦‚æœé…ç½®äº†OpenAI APIï¼Œåˆ™åˆå§‹åŒ–LLM
        if OPENAI_API_KEY and OPENAI_API_KEY.strip():
            try:
                self.llm = ChatOpenAI(
                    api_key=OPENAI_API_KEY,
                    base_url=OPENAI_BASE_URL,
                    temperature=0.1,
                    model="gpt-3.5-turbo"
                )
                
                # åˆ›å»ºæç¤ºæ¨¡æ¿
                system_template = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ä»…åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œå†…å®¹ç‰‡æ®µ
5. ç”¨ä¸­æ–‡å›ç­”

æ–‡æ¡£å†…å®¹ï¼š
{context}"""

                human_template = "é—®é¢˜ï¼š{question}"
                
                chat_prompt = ChatPromptTemplate([
                    ("system", system_template),
                    ("human", human_template),
                ])
                
                # åˆ›å»ºè¾“å‡ºè§£æå™¨
                output_parser = StrOutputParser()
                
                # åˆ›å»ºå¤„ç†é“¾
                self.chain = chat_prompt | self.llm | output_parser
                
                print("âœ… LangChain + OpenAI åˆå§‹åŒ–æˆåŠŸ")
                
            except Exception as e:
                print(f"âš ï¸ LLMåˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼: {e}")
                self.llm = None
                self.chain = None
        else:
            print("âš ï¸ æœªé…ç½®OpenAI API Keyï¼Œå°†ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼")
    
    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        if not context_chunks:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, chunk in enumerate(context_chunks, 1):
            doc_name = chunk['document'].filename
            content = chunk['chunk'].content
            similarity = chunk['similarity']
            context_parts.append(f"æ–‡æ¡£{i}: {doc_name} (ç›¸ä¼¼åº¦: {similarity:.2f})\nå†…å®¹: {content}")
        
        context = "\n\n".join(context_parts)
        
        # å¦‚æœæœ‰LLMï¼Œä½¿ç”¨æ™ºèƒ½å›ç­”
        if self.chain:
            try:
                answer = await self._llm_answer(query, context)
                return answer
            except Exception as e:
                print(f"LLMå›ç­”å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼: {e}")
                return self._simple_answer(query, context_chunks)
        else:
            # ä½¿ç”¨ç®€å•å›ç­”
            return self._simple_answer(query, context_chunks)
    
    async def _llm_answer(self, query: str, context: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”"""
        try:
            # è°ƒç”¨LangChainå¤„ç†é“¾
            response = await self.chain.ainvoke({
                "question": query,
                "context": context
            })
            return response
        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """ç®€å•çš„åŸºäºå…³é”®è¯çš„å›ç­”ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if not context_chunks:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        
        # æå–æœ€ç›¸å…³çš„æ–‡æ¡£ä¿¡æ¯
        best_chunk = context_chunks[0]
        document_name = best_chunk['document'].filename
        content = best_chunk['chunk'].content
        similarity = best_chunk['similarity']
        
        answer = f"""æ ¹æ®æ–‡æ¡£ã€Š{document_name}ã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼ˆç›¸ä¼¼åº¦: {similarity:.2f}ï¼‰ï¼š

{content}

---
ğŸ’¡ æç¤ºï¼šå½“å‰ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼ã€‚å¦‚éœ€æ›´æ™ºèƒ½çš„å›ç­”ï¼Œè¯·é…ç½®OpenAI API Keyã€‚"""
        
        return answer


# å…¨å±€å®ä¾‹
document_processor = DocumentProcessor()
vector_search = VectorSearch()
rag_generator = RAGGenerator()
