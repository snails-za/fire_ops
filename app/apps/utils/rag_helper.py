"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—

è¯¥æ¨¡å—åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
1. DocumentProcessor: æ–‡æ¡£å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡æ¡£å†…å®¹æå–ã€åˆ†å—å’Œå‘é‡åŒ–
2. VectorSearch: å‘é‡æœç´¢å¼•æ“ï¼ŒåŸºäºChromaæ•°æ®åº“è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
3. RAGGenerator: RAGç”Ÿæˆå™¨ï¼Œé›†æˆLangChainå’ŒOpenAIè¿›è¡Œæ™ºèƒ½é—®ç­”

æŠ€æœ¯æ ˆï¼š
- æ–‡æ¡£å¤„ç†: PyPDF, python-docx, openpyxl
- æ–‡æœ¬åˆ†å‰²: LangChain RecursiveCharacterTextSplitter
- å‘é‡åŒ–: Sentence Transformers
- å‘é‡å­˜å‚¨: ChromaDB
- æ™ºèƒ½é—®ç­”: LangChain + OpenAI GPT
"""

import os
import traceback
import uuid
from typing import List, Dict, Any, Optional

import chromadb
import openpyxl
import pypdf
import pytesseract
import cv2
import numpy as np
from PIL import Image
from pdf2image import convert_from_path
from chromadb.config import Settings as ChromaSettings
from docx import Document as DocxDocument
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import SentenceTransformer

from apps.models.document import Document as DocumentModel, DocumentChunk
from config import (
    CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION, EMBEDDING_MODEL, 
    HF_HOME, HF_OFFLINE, OPENAI_API_KEY, OPENAI_BASE_URL, SIMILARITY_THRESHOLD,
    OCR_ENABLED, OCR_AUTO_FALLBACK, OCR_MIN_TEXT_LENGTH, OCR_MAX_FILE_SIZE
)

# RAGç³»ç»Ÿå·¥å…·å‡½æ•°


def get_local_model_path(model_name: str, cache_folder: str) -> Optional[str]:
    """
    è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„
    
    Args:
        model_name: HuggingFaceæ¨¡å‹åç§°
        cache_folder: ç¼“å­˜æ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–None
    """
    # HuggingFaceå°† '/' è½¬æ¢ä¸º '--'
    local_model_name = model_name.replace('/', '--')
    local_model_path = os.path.join(cache_folder, f"models--{local_model_name}")
    
    if os.path.exists(local_model_path):
        # æ£€æŸ¥æ˜¯å¦æœ‰snapshotsç›®å½•
        snapshots_dir = os.path.join(local_model_path, "snapshots")
        if os.path.exists(snapshots_dir):
            # è·å–æœ€æ–°çš„snapshot
            snapshots = [d for d in os.listdir(snapshots_dir) 
                        if os.path.isdir(os.path.join(snapshots_dir, d))]
            if snapshots:
                latest_snapshot = os.path.join(snapshots_dir, snapshots[0])
                return latest_snapshot
        
        # å¦‚æœæ²¡æœ‰snapshotsï¼Œç›´æ¥è¿”å›æ¨¡å‹ç›®å½•
        return local_model_path
    
    return None


class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†å™¨ - è´Ÿè´£æ–‡æ¡£å†…å®¹æå–ã€åˆ†å—å’Œå‘é‡åŒ–
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€DOCXã€Excelã€TXTï¼‰
    2. æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    3. ç”Ÿæˆé«˜è´¨é‡å‘é‡åµŒå…¥
    4. ä¸æ•°æ®åº“å’Œå‘é‡å­˜å‚¨åŒæ­¥
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        é…ç½®ç»„ä»¶ï¼š
        1. æ–‡æœ¬åˆ†å‰²å™¨ï¼š1000å­—ç¬¦å—å¤§å°ï¼Œ200å­—ç¬¦é‡å 
        2. å‘é‡åµŒå…¥æ¨¡å‹ï¼šSentence Transformers
        3. ChromaDBå‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        """
        try:
            # é…ç½®æ–‡æœ¬åˆ†å‰²å™¨ - å¹³è¡¡å—å¤§å°å’Œè¯­ä¹‰å®Œæ•´æ€§
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,      # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
                chunk_overlap=200,    # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
                length_function=len,  # ä½¿ç”¨å­—ç¬¦é•¿åº¦è®¡ç®—
            )
            
            # é…ç½®HuggingFaceç¯å¢ƒå˜é‡
            os.environ["HF_HOME"] = HF_HOME
            os.environ["TRANSFORMERS_CACHE"] = HF_HOME
            os.environ["HF_HUB_CACHE"] = HF_HOME
            
            if HF_OFFLINE:
                # ç¦»çº¿æ¨¡å¼é…ç½®
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"
            
            # å…³é—­Chromaé¥æµ‹ï¼Œé¿å…ç½‘ç»œè¯·æ±‚å’Œé”™è¯¯
            os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
            os.environ.setdefault("CHROMA_TELEMETRY", "false")
            
            # åˆå§‹åŒ–å‘é‡åµŒå…¥æ¨¡å‹
            local_model_path = get_local_model_path(EMBEDDING_MODEL, HF_HOME)
            
            if local_model_path and HF_OFFLINE:
                self.embedding_model = SentenceTransformer(local_model_path)
            else:
                self.embedding_model = SentenceTransformer(
                    EMBEDDING_MODEL, 
                    cache_folder=HF_HOME
                )
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ
            os.makedirs(CHROMA_PERSIST_DIRECTORY, exist_ok=True)
            self.chroma_client = chromadb.PersistentClient(
                path=CHROMA_PERSIST_DIRECTORY, 
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            self.collection = self.chroma_client.get_or_create_collection(
                name=CHROMA_COLLECTION, 
                metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            )
            
        except Exception as e:
            raise Exception(f"DocumentProcessoråˆå§‹åŒ–å¤±è´¥: {e}")
        
    async def process_document(self, document_id: int, file_path: str, file_type: str) -> bool:
        """
        å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡åµŒå…¥
        
        å¤„ç†æµç¨‹ï¼š
        1. æå–æ–‡æ¡£å†…å®¹
        2. æ™ºèƒ½åˆ†å—å¤„ç†
        3. ç”Ÿæˆå‘é‡åµŒå…¥
        4. å­˜å‚¨åˆ°ChromaDB
        5. æ›´æ–°æ•°æ®åº“çŠ¶æ€
        
        Args:
            document_id: æ–‡æ¡£ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        document = None
        try:
            # 1. æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤„ç†ä¸­
            document = await DocumentModel.get(id=document_id)
            document.status = "processing"
            await document.save()
            
            # 2. æå–æ–‡æ¡£å†…å®¹
            content = await self._extract_content(file_path, file_type)
            if not content or not content.strip():
                raise Exception("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–")
            
            # æ›´æ–°æ–‡æ¡£å†…å®¹åˆ°æ•°æ®åº“
            document.content = content
            await document.save()
            
            # 3. æ™ºèƒ½åˆ†å—å¤„ç†
            chunks = self.text_splitter.split_text(content)
            if not chunks:
                raise Exception("æ–‡æ¡£åˆ†å—å¤±è´¥")
            
            # 4. åˆ›å»ºåˆ†å—è®°å½•
            chunk_objects = []
            for i, chunk_text in enumerate(chunks):
                chunk = await DocumentChunk.create(
                    document_id=document_id,
                    chunk_index=i,
                    content=chunk_text,
                    content_length=len(chunk_text),
                    metadata={"chunk_index": i}
                )
                chunk_objects.append(chunk)
            
            # 5. ç”Ÿæˆå‘é‡åµŒå…¥
            embeddings = self.embedding_model.encode(chunks)
            
            # 6. å­˜å‚¨å‘é‡åˆ°ChromaDB
            if len(chunk_objects) > 0:
                ids = []
                metadatas = []
                vectors = []
                
                for i, (chunk, embedding) in enumerate(zip(chunk_objects, embeddings)):
                    vector_id = f"doc_{document_id}_chunk_{i}_{uuid.uuid4().hex[:8]}"
                    ids.append(vector_id)
                    metadatas.append({
                        "document_id": document_id,
                        "chunk_id": chunk.id,
                        "chunk_index": i,
                    })
                    vectors.append(embedding.tolist())
                
                # æ‰¹é‡æ·»åŠ åˆ°ChromaDB
                self.collection.add(
                    ids=ids, 
                    embeddings=vectors, 
                    metadatas=metadatas
                )
            
            # 7. æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå®Œæˆ
            document.status = "completed"
            await document.save()
            
            return True
            
        except Exception as e:
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
            try:
                if document is None:
                    document = await DocumentModel.get(id=document_id)
                document.status = "failed"
                document.error_message = str(e)
                await document.save()
            except Exception:
                pass  # å¿½ç•¥ä¿å­˜é”™è¯¯çŠ¶æ€çš„å¼‚å¸¸
            
            return False
    
    async def _extract_content(self, file_path: str, file_type: str) -> str:
        """
        æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æ¡£å†…å®¹
        
        æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š
        - PDF: ä½¿ç”¨PyPDFæå–æ–‡æœ¬
        - DOCX/DOC: ä½¿ç”¨python-docxæå–æ®µè½
        - XLSX/XLS: ä½¿ç”¨openpyxlæå–è¡¨æ ¼æ•°æ®
        - TXT: ç›´æ¥è¯»å–æ–‡æœ¬å†…å®¹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            if not os.path.exists(file_path):
                raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            if file_type == "pdf":
                return await self._extract_pdf_content(file_path)
            elif file_type in ["docx", "doc"]:
                return await self._extract_docx_content(file_path)
            elif file_type in ["xlsx", "xls"]:
                return await self._extract_excel_content(file_path)
            elif file_type == "txt":
                return await self._extract_txt_content(file_path)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                
        except Exception as e:
            raise Exception(f"æå–æ–‡æ¡£å†…å®¹å¤±è´¥: {str(e)}")
    
    async def _extract_pdf_content(self, file_path: str) -> str:
        """
        æ™ºèƒ½PDFå†…å®¹æå– - æ ¹æ®é…ç½®å’Œæ–‡æ¡£ç‰¹å¾é€‰æ‹©æœ€ä½³ç­–ç•¥
        
        å¤„ç†ç­–ç•¥ï¼š
        1. æ€»æ˜¯å…ˆå°è¯•æ–‡æœ¬æå–ï¼ˆå¿«é€Ÿã€å‡†ç¡®ï¼‰
        2. æ ¹æ®é…ç½®å’Œç»“æœè´¨é‡å†³å®šæ˜¯å¦ä½¿ç”¨OCR
        3. æä¾›æ¸…æ™°çš„å¤„ç†çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯
        
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
            
        Raises:
            Exception: å½“æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥æ—¶
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            print(f"ğŸ“„ å¼€å§‹å¤„ç†PDFæ–‡æ¡£ï¼Œæ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f}MB")
            
            # ç¬¬ä¸€æ­¥ï¼šæ€»æ˜¯å…ˆå°è¯•æ–‡æœ¬æå–
            text_content = await self._extract_pdf_text(file_path)
            text_length = len(text_content.strip()) if text_content else 0
            
            # åˆ¤æ–­æ–‡æœ¬æå–è´¨é‡
            is_text_sufficient = text_length >= OCR_MIN_TEXT_LENGTH
            
            if is_text_sufficient:
                print(f"âœ… PDFæ–‡æœ¬æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {text_length} å­—ç¬¦")
                return text_content
            
            # ç¬¬äºŒæ­¥ï¼šå†³å®šæ˜¯å¦ä½¿ç”¨OCR
            if not OCR_ENABLED:
                if text_content:
                    print(f"âš ï¸ OCRåŠŸèƒ½æœªå¯ç”¨ï¼Œè¿”å›å·²æå–çš„æ–‡æœ¬å†…å®¹ ({text_length} å­—ç¬¦)")
                    return text_content
                else:
                    raise Exception("PDFæ— æ³•æå–æ–‡æœ¬å†…å®¹ï¼Œä¸”OCRåŠŸèƒ½æœªå¯ç”¨ã€‚è¯·å¯ç”¨OCRæˆ–æä¾›æ–‡æœ¬æ ¼å¼çš„PDFã€‚")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
            if file_size > OCR_MAX_FILE_SIZE:
                if text_content:
                    print(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ ({file_size / 1024 / 1024:.2f}MB > {OCR_MAX_FILE_SIZE / 1024 / 1024}MB)ï¼Œè·³è¿‡OCRå¤„ç†")
                    return text_content
                else:
                    raise Exception(f"PDFæ–‡ä»¶è¿‡å¤§ ({file_size / 1024 / 1024:.2f}MB)ï¼Œæ— æ³•è¿›è¡ŒOCRå¤„ç†ã€‚è¯·æä¾›æ›´å°çš„æ–‡ä»¶æˆ–æ–‡æœ¬æ ¼å¼çš„PDFã€‚")
            
            # å¦‚æœä¸æ˜¯è‡ªåŠ¨é™çº§æ¨¡å¼ï¼Œä¸”æœ‰ä¸€äº›æ–‡æœ¬å†…å®¹ï¼Œå…ˆè¿”å›æ–‡æœ¬å†…å®¹
            if not OCR_AUTO_FALLBACK and text_content:
                print(f"â„¹ï¸ æ£€æµ‹åˆ°å°‘é‡æ–‡æœ¬å†…å®¹ ({text_length} å­—ç¬¦)ï¼ŒOCRéœ€æ‰‹åŠ¨å¯ç”¨")
                return text_content
            
            # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡ŒOCRå¤„ç†
            print(f"ğŸ“¸ æ–‡æœ¬å†…å®¹ä¸è¶³ ({text_length} < {OCR_MIN_TEXT_LENGTH})ï¼Œå¼€å§‹OCRå¤„ç†...")
            print("â³ OCRå¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            ocr_content = await self._extract_pdf_with_ocr(file_path)
            ocr_length = len(ocr_content.strip()) if ocr_content else 0
            
            if ocr_content and ocr_length > 10:
                print(f"âœ… PDF OCRå¤„ç†æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {ocr_length} å­—ç¬¦")
                return ocr_content
            
            # æœ€åçš„é™çº§å¤„ç†
            if text_content:
                print(f"âš ï¸ OCRå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬æå–ç»“æœ ({text_length} å­—ç¬¦)")
                return text_content
                
            raise Exception("PDFæ–‡æ¡£å¤„ç†å¤±è´¥ï¼šæ–‡æœ¬æå–å’ŒOCRè¯†åˆ«å‡æœªè·å¾—æœ‰æ•ˆå†…å®¹")
            
        except Exception as e:
            print(f"âŒ PDFå¤„ç†å‡ºé”™: {str(e)}")
            raise Exception(f"PDFå†…å®¹æå–å¤±è´¥: {str(e)}")
    
    async def _extract_pdf_text(self, file_path: str) -> str:
        """
        ä½¿ç”¨PyPDFç›´æ¥æå–PDFæ–‡æœ¬å†…å®¹
        
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            content = ""
            with open(file_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text.strip():  # åªæ·»åŠ éç©ºé¡µé¢
                        content += f"\n--- ç¬¬ {page_num} é¡µ ---\n"
                        content += page_text + "\n"
                        
            return content.strip()
            
        except Exception as e:
            print(f"PDFæ–‡æœ¬æå–å‡ºé”™: {str(e)}")
            return ""
    
    async def _extract_pdf_with_ocr(self, file_path: str) -> str:
        """
        ä½¿ç”¨OCRæŠ€æœ¯æå–PDFä¸­çš„å›¾ç‰‡æ–‡å­—
        
        åŒ…å«å®Œæ•´çš„ä¾èµ–æ£€æŸ¥å’Œé”™è¯¯å¤„ç†
        
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: OCRè¯†åˆ«çš„æ–‡æœ¬å†…å®¹
            
        Raises:
            Exception: å½“OCRä¾èµ–ç¼ºå¤±æˆ–å¤„ç†å¤±è´¥æ—¶
        """
        try:
            # æ£€æŸ¥OCRä¾èµ–
            self._check_ocr_dependencies()
            
            # å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡
            print("ğŸ”„ æ­£åœ¨å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡...")
            try:
                images = convert_from_path(file_path, dpi=200)  # é™ä½DPIå¹³è¡¡è´¨é‡å’Œæ€§èƒ½
            except Exception as e:
                if "poppler" in str(e).lower():
                    raise Exception("ç¼ºå°‘popplerä¾èµ–ã€‚è¯·è¿è¡Œ: brew install poppler (macOS) æˆ– apt-get install poppler-utils (Ubuntu)")
                raise Exception(f"PDFè½¬å›¾ç‰‡å¤±è´¥: {str(e)}")
            
            if not images:
                raise Exception("PDFè½¬æ¢åæœªè·å¾—ä»»ä½•å›¾ç‰‡é¡µé¢")
            
            content = ""
            total_pages = len(images)
            successful_pages = 0
            
            print(f"ğŸ“„ å¼€å§‹OCRå¤„ç† {total_pages} é¡µ...")
            
            for page_num, image in enumerate(images, 1):
                try:
                    print(f"ğŸ” å¤„ç†ç¬¬ {page_num}/{total_pages} é¡µ...")
                    
                    # å›¾åƒé¢„å¤„ç†
                    processed_image = self._preprocess_image_for_ocr(image)
                    
                    # OCRè¯†åˆ« - ä½¿ç”¨ç®€å•å¯é çš„é…ç½®
                    page_text = pytesseract.image_to_string(
                        processed_image, 
                        lang='chi_sim+eng',
                        config='--oem 3 --psm 6'
                    )
                    
                    if page_text.strip():
                        content += f"\n--- ç¬¬ {page_num} é¡µ (OCR) ---\n"
                        content += page_text.strip() + "\n"
                        successful_pages += 1
                        
                except Exception as page_error:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µOCRå¤„ç†å¤±è´¥: {str(page_error)}")
                    continue
            
            if successful_pages == 0:
                raise Exception("æ‰€æœ‰é¡µé¢çš„OCRå¤„ç†å‡å¤±è´¥")
            
            print(f"âœ… OCRå¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_pages}/{total_pages} é¡µ")
            return content.strip()
            
        except Exception as e:
            error_msg = str(e)
            if "tesseract" in error_msg.lower():
                error_msg = "ç¼ºå°‘Tesseract OCRå¼•æ“ã€‚è¯·è¿è¡Œ: brew install tesseract (macOS) æˆ– apt-get install tesseract-ocr (Ubuntu)"
            elif "chi_sim" in error_msg.lower():
                error_msg = "ç¼ºå°‘ä¸­æ–‡è¯­è¨€åŒ…ã€‚è¯·è¿è¡Œ: brew install tesseract-lang (macOS) æˆ– apt-get install tesseract-ocr-chi-sim (Ubuntu)"
            
            print(f"âŒ PDF OCRå¤„ç†å¤±è´¥: {error_msg}")
            raise Exception(f"OCRå¤„ç†å¤±è´¥: {error_msg}")
    
    def _check_ocr_dependencies(self):
        """
        æ£€æŸ¥OCRæ‰€éœ€çš„ä¾èµ–æ˜¯å¦å¯ç”¨
        
        Raises:
            Exception: å½“ä¾èµ–ç¼ºå¤±æ—¶
        """
        try:
            # æ£€æŸ¥pytesseract
            import pytesseract
            
            # å°è¯•è®¾ç½®tesseractè·¯å¾„ï¼ˆmacOS Homebrewé»˜è®¤è·¯å¾„ï¼‰
            import shutil
            tesseract_path = shutil.which('tesseract')
            if tesseract_path:
                pytesseract.pytesseract.tesseract_cmd = tesseract_path
                print(f"ğŸ”§ è®¾ç½®Tesseractè·¯å¾„: {tesseract_path}")
            
            # æ£€æŸ¥Tesseractå¯æ‰§è¡Œæ–‡ä»¶
            version = pytesseract.get_tesseract_version()
            print(f"âœ… Tesseractç‰ˆæœ¬: {version}")
            
            # æ£€æŸ¥æ”¯æŒçš„è¯­è¨€
            languages = pytesseract.get_languages()
            print(f"ğŸ“‹ æ”¯æŒçš„è¯­è¨€: {len(languages)} ç§")
            
            if 'chi_sim' not in languages:
                raise Exception("Tesseractç¼ºå°‘ä¸­æ–‡ç®€ä½“è¯­è¨€åŒ…ã€‚è¯·è¿è¡Œ: brew install tesseract-lang")
            if 'eng' not in languages:
                raise Exception("Tesseractç¼ºå°‘è‹±æ–‡è¯­è¨€åŒ…")
            
            print("âœ… OCRä¾èµ–æ£€æŸ¥é€šè¿‡")
                
        except ImportError:
            raise Exception("pytesseractåŒ…æœªå®‰è£…")
        except Exception as e:
            error_str = str(e).lower()
            if "tesseract is not installed" in error_str or "tesseract not found" in error_str:
                raise Exception("Tesseract OCRå¼•æ“æœªå®‰è£…æˆ–æœªåœ¨PATHä¸­")
            raise e
    
    
    def _preprocess_image_for_ocr(self, pil_image: Image.Image) -> Image.Image:
        """
        ç®€å•çš„å›¾åƒé¢„å¤„ç†ï¼Œæé«˜OCRè¯†åˆ«å‡†ç¡®æ€§
        
        Args:
            pil_image: PILå›¾åƒå¯¹è±¡
            
        Returns:
            Image.Image: é¢„å¤„ç†åçš„å›¾åƒ
        """
        try:
            # ç®€å•çš„ç°åº¦è½¬æ¢
            if pil_image.mode != 'L':
                gray_image = pil_image.convert('L')
            else:
                gray_image = pil_image
            
            # å¦‚æœå›¾åƒå¤ªå°ï¼Œç¨å¾®æ”¾å¤§
            width, height = gray_image.size
            if width < 800 or height < 800:
                scale_factor = max(800 / width, 800 / height)
                new_width = int(width * scale_factor)
                new_height = int(height * scale_factor)
                gray_image = gray_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            return gray_image
            
        except Exception as e:
            print(f"âš ï¸ å›¾åƒé¢„å¤„ç†å‡ºé”™: {str(e)}")
            # å¦‚æœé¢„å¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå›¾
            return pil_image
    
    async def _extract_docx_content(self, file_path: str) -> str:
        """
        æå–DOCXæ–‡æ¡£å†…å®¹
        
        Args:
            file_path: DOCXæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            doc = DocxDocument(file_path)
            content = ""
            paragraph_count = 0
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():  # åªæ·»åŠ éç©ºæ®µè½
                    content += paragraph.text + "\n"
                    paragraph_count += 1
            
            if not content.strip():
                raise Exception("DOCXæ–‡æ¡£æ— æ³•æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å†…å®¹")
                
            return content.strip()
            
        except Exception as e:
            raise Exception(f"DOCXå†…å®¹æå–å¤±è´¥: {str(e)}")
    
    async def _extract_excel_content(self, file_path: str) -> str:
        """
        æå–Excelæ–‡æ¡£å†…å®¹
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            workbook = openpyxl.load_workbook(file_path, data_only=True)
            content = ""
            total_rows = 0
            
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                content += f"\n=== å·¥ä½œè¡¨: {sheet_name} ===\n"
                
                # è·å–æœ‰æ•°æ®çš„è¡Œ
                rows_with_data = []
                for row in sheet.iter_rows(values_only=True):
                    if any(cell is not None and str(cell).strip() for cell in row):
                        row_text = "\t".join([
                            str(cell).strip() if cell is not None else "" 
                            for cell in row
                        ])
                        rows_with_data.append(row_text)
                        total_rows += 1
                
                if rows_with_data:
                    content += "\n".join(rows_with_data) + "\n"
                else:
                    content += "ï¼ˆæ­¤å·¥ä½œè¡¨æ— æ•°æ®ï¼‰\n"
            
            if not content.strip():
                raise Exception("Excelæ–‡æ¡£æ— æ³•æå–åˆ°æœ‰æ•ˆå†…å®¹")
                
            return content.strip()
            
        except Exception as e:
            raise Exception(f"Excelå†…å®¹æå–å¤±è´¥: {str(e)}")
    
    async def _extract_txt_content(self, file_path: str) -> str:
        """
        æå–TXTæ–‡æ¡£å†…å®¹
        
        Args:
            file_path: TXTæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            # å°è¯•å¤šç§ç¼–ç æ ¼å¼
            encodings = ['utf-8', 'gbk', 'gb2312', 'big5', 'latin1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        content = file.read()
                        if content.strip():
                            return content.strip()
                except UnicodeDecodeError:
                    continue
            
            raise Exception("æ— æ³•ä½¿ç”¨æ”¯æŒçš„ç¼–ç æ ¼å¼è¯»å–æ–‡æœ¬æ–‡ä»¶")
            
        except Exception as e:
            raise Exception(f"TXTå†…å®¹æå–å¤±è´¥: {str(e)}")


class VectorSearch:
    """
    å‘é‡æœç´¢å¼•æ“ - åŸºäºChromaDBçš„è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    2. å‘é‡æ•°æ®ç®¡ç†
    3. æœç´¢ç»“æœæ’åºå’Œè¿‡æ»¤
    4. ä¸æ•°æ®åº“æ•°æ®å…³è”
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        
        é…ç½®ç»„ä»¶ï¼š
        1. å‘é‡åµŒå…¥æ¨¡å‹ï¼ˆä¸DocumentProcessorä¿æŒä¸€è‡´ï¼‰
        2. ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ
        """
        try:
            # é…ç½®HuggingFaceç¯å¢ƒ
            os.environ["HF_HOME"] = HF_HOME
            os.environ["TRANSFORMERS_CACHE"] = HF_HOME
            os.environ["HF_HUB_CACHE"] = HF_HOME
            
            if HF_OFFLINE:
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"
            
            # å…³é—­Chromaé¥æµ‹
            os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
            os.environ.setdefault("CHROMA_TELEMETRY", "false")
            
            # åˆå§‹åŒ–å‘é‡åµŒå…¥æ¨¡å‹ï¼ˆä¸DocumentProcessorä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼‰
            local_model_path = get_local_model_path(EMBEDDING_MODEL, HF_HOME)
            
            if local_model_path and HF_OFFLINE:
                self.embedding_model = SentenceTransformer(local_model_path)
            else:
                self.embedding_model = SentenceTransformer(
                    EMBEDDING_MODEL, 
                    cache_folder=HF_HOME
                )
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(
                path=CHROMA_PERSIST_DIRECTORY, 
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            self.collection = self.chroma_client.get_or_create_collection(
                name=CHROMA_COLLECTION, 
                metadata={"hnsw:space": "cosine"}
            )
            
        except Exception as e:
            raise Exception(f"VectorSearchåˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def search_similar_chunks(self, query: str, top_k: int = 5, use_threshold: bool = True) -> List[Dict[str, Any]]:
        """
        æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£å—
        
        æœç´¢æµç¨‹ï¼š
        1. å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        2. åœ¨ChromaDBä¸­è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
        3. ä»æ•°æ®åº“è·å–å®Œæ•´çš„æ–‡æ¡£å’Œå—ä¿¡æ¯
        4. è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°å¹¶æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            
        Returns:
            List[Dict]: ç›¸ä¼¼æ–‡æ¡£å—åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æ¡£ã€å—å’Œç›¸ä¼¼åº¦ä¿¡æ¯
        """
        try:
            if not query or not query.strip():
                return []
            
            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode([query.strip()])[0]
            
            # 2. åœ¨ChromaDBä¸­æœç´¢ç›¸ä¼¼å‘é‡
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()], 
                n_results=min(top_k, 20)  # é™åˆ¶æœ€å¤§æœç´¢æ•°é‡
            )
            
            # 3. è§£ææœç´¢ç»“æœ
            ids = results.get('ids', [[]])[0]
            metadatas = results.get('metadatas', [[]])[0]
            distances = results.get('distances', [[]])[0] or []
            
            # 4. ä»æ•°æ®åº“è·å–å®Œæ•´ä¿¡æ¯å¹¶æ„å»ºç»“æœ
            all_similarities = []  # å­˜å‚¨æ‰€æœ‰ç»“æœ
            filtered_similarities = []  # å­˜å‚¨è¿‡æ»¤åçš„ç»“æœ
            
            for i, (vector_id, metadata) in enumerate(zip(ids, metadatas)):
                try:
                    # è·å–æ–‡æ¡£å—ä¿¡æ¯
                    chunk_id = metadata.get('chunk_id')
                    if not chunk_id:
                        continue
                    
                    chunk = await DocumentChunk.get_or_none(id=chunk_id)
                    if not chunk:
                        continue
                    
                    # è·å–å…³è”çš„æ–‡æ¡£
                    document = await chunk.document
                    if not document:
                        continue
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
                    distance = distances[i] if i < len(distances) else 1.0
                    similarity = max(0.0, 1.0 - float(distance))
                    
                    result_item = {
                        'vector_id': vector_id,
                        'similarity': similarity,
                        'chunk': chunk,
                        'document': document,
                        'metadata': metadata,
                        'above_threshold': similarity >= SIMILARITY_THRESHOLD
                    }
                    
                    all_similarities.append(result_item)
                    
                    # å¦‚æœä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåªä¿ç•™ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ç»“æœ
                    if not use_threshold or similarity >= SIMILARITY_THRESHOLD:
                        filtered_similarities.append(result_item)
                    
                except Exception as e:
                    print(f"å¤„ç†æœç´¢ç»“æœé¡¹å¤±è´¥ (chunk_id: {metadata.get('chunk_id', 'unknown')}): {e}")
                    continue
            
            # 5. æ™ºèƒ½é€‰æ‹©è¿”å›ç»“æœ
            if use_threshold and filtered_similarities:
                # æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œè¿”å›è¿‡æ»¤åçš„ç»“æœ
                filtered_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                return filtered_similarities[:top_k]
            elif use_threshold and not filtered_similarities and all_similarities:
                # æ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œä½†æœ‰æœç´¢ç»“æœï¼Œè¿”å›æœ€ç›¸ä¼¼çš„å‡ ä¸ªå¹¶æ ‡è®°
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                # å–å‰å‡ ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœï¼Œä½†æ ‡è®°ä¸ºä½ç›¸ä¼¼åº¦
                return all_similarities[:min(top_k, 3)]  # æœ€å¤šè¿”å›3ä¸ªä½ç›¸ä¼¼åº¦ç»“æœ
            else:
                # ä¸ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œè¿”å›æ‰€æœ‰ç»“æœ
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                return all_similarities[:top_k]
            
        except Exception as e:
            print(f"æœç´¢ç›¸ä¼¼æ–‡æ¡£å—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def delete_document_vectors(self, document_id: int):
        """
        åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡æ•°æ®
        
        Args:
            document_id: æ–‡æ¡£ID
        """
        try:
            # é€šè¿‡metadataæ¡ä»¶åˆ é™¤
            self.collection.delete(where={"document_id": document_id})
            
        except Exception as e:
            raise Exception(f"åˆ é™¤æ–‡æ¡£ {document_id} å‘é‡æ•°æ®å¤±è´¥: {e}")

    async def count_vectors(self) -> int:
        """
        ç»Ÿè®¡å‘é‡æ•°æ®åº“ä¸­çš„å‘é‡æ€»æ•°
        
        Returns:
            int: å‘é‡æ€»æ•°
        """
        try:
            count = self.collection.count()
            return int(count) if isinstance(count, (int, float)) else 0
            
        except Exception:
            return 0


class RAGGenerator:
    """
    RAGç”Ÿæˆå™¨ - æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. é›†æˆå‘é‡æœç´¢å’ŒLLMç”Ÿæˆ
    2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
    3. æ”¯æŒå¤šç§å›ç­”æ¨¡å¼ï¼ˆLLMæ™ºèƒ½å›ç­” / ç®€å•å›ç­”ï¼‰
    4. æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—®ç­”ä½“éªŒ
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–RAGç”Ÿæˆå™¨
        
        ç»„ä»¶ï¼š
        1. VectorSearch: å‘é‡æœç´¢å¼•æ“
        2. LangChain + OpenAI: æ™ºèƒ½é—®ç­”é“¾ï¼ˆå¯é€‰ï¼‰
        3. å¤‡ç”¨ç®€å•å›ç­”æ¨¡å¼
        """
        try:
            # åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
            self.vector_search = VectorSearch()
            
            # åˆå§‹åŒ–LLMç»„ä»¶
            self.llm = None
            self.chain = None
            self.llm_available = False
            
            # æ£€æŸ¥OpenAIé…ç½®å¹¶åˆå§‹åŒ–LLM
            if OPENAI_API_KEY and OPENAI_API_KEY.strip():
                try:
                    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
                    self.llm = ChatOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿å›ç­”çš„ä¸€è‡´æ€§
                        model="gpt-3.5-turbo",
                        max_tokens=2000  # é™åˆ¶å›ç­”é•¿åº¦
                    )
                    
                    # åˆ›å»ºç³»ç»Ÿæç¤ºæ¨¡æ¿
                    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„ï¼š
   - ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜ç»„ç»‡å†…å®¹
   - é‡è¦ä¿¡æ¯ç”¨**ç²—ä½“**æ ‡è®°
   - ä½¿ç”¨é¡¹ç›®ç¬¦å·(â€¢)æˆ–æ•°å­—åˆ—è¡¨å±•ç¤ºè¦ç‚¹
   - æ¯ä¸ªæ®µè½ä¸“æ³¨ä¸€ä¸ªä¸»é¢˜ï¼Œæ®µè½é—´ç•™ç©ºè¡Œ
   - å¤æ‚å†…å®¹ä½¿ç”¨è¡¨æ ¼æˆ–ç»“æ„åŒ–æ ¼å¼
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œå…³é”®å†…å®¹ç‰‡æ®µï¼Œæ ¼å¼ï¼šã€Œæ–‡æ¡£åï¼šå…·ä½“å†…å®¹ã€
5. å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£æä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶æ ‡æ˜ä¿¡æ¯æ¥æº
6. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚å½“ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}"""

                    human_template = "é—®é¢˜ï¼š{question}"
                    
                    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
                    chat_prompt = ChatPromptTemplate([
                        ("system", system_template),
                        ("human", human_template),
                    ])
                    
                    # åˆ›å»ºè¾“å‡ºè§£æå™¨
                    output_parser = StrOutputParser()
                    
                    # åˆ›å»ºLangChainå¤„ç†é“¾
                    self.chain = chat_prompt | self.llm | output_parser
                    self.llm_available = True
                    
                except Exception:
                    self.llm = None
                    self.chain = None
                    self.llm_available = False
            else:
                self.llm_available = False
                
        except Exception as e:
            raise Exception(f"RAGGeneratoråˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        å›ç­”ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨LLMæ™ºèƒ½å›ç­”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. é™çº§åˆ°ç®€å•å›ç­”æ¨¡å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        3. æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ¥æºå¼•ç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                content = chunk['chunk'].content
                similarity = chunk['similarity']
                
                # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
                context_part = f"""æ–‡æ¡£ {i}: {doc_name}
ç›¸ä¼¼åº¦: {similarity:.2f}
å†…å®¹: {content}"""
                context_parts.append(context_part)
            
            context = "\n\n" + "="*50 + "\n\n".join(context_parts)
            
            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    answer = await self._llm_answer(query, context)
                    
                    # ä¸å†è‡ªåŠ¨æ·»åŠ æ¥æºä¿¡æ¯ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦æ˜¾ç¤º
                    return answer
                    
                except Exception:
                    return self._simple_answer(query, context_chunks)
            else:
                return self._simple_answer(query, context_chunks)
                
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    async def generate_answer_stream(self, query: str, context_chunks: List[Dict[str, Any]]):
        """
        æµå¼ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ–‡æ¡£ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk.get('document_name', 'æœªçŸ¥æ–‡æ¡£')
                content = chunk.get('content', '')
                context_parts.append(f"æ–‡æ¡£{i}: {doc_name}\nå†…å®¹: {content}")
            
            context = "\n\n" + "="*50 + "\n\n".join(context_parts)
            
            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    async for chunk in self._llm_answer_stream(query, context):
                        yield chunk
                except Exception as e:
                    print(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {e}")
                    # é™çº§åˆ°ç®€å•å›ç­”
                    simple_answer = self._simple_answer(query, context_chunks)
                    yield simple_answer
            else:
                # éLLMæ¨¡å¼ï¼Œç›´æ¥è¿”å›ç®€å•å›ç­”
                simple_answer = self._simple_answer(query, context_chunks)
                yield simple_answer
                
        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def _llm_answer_stream(self, query: str, context: str):
        """
        ä½¿ç”¨LangChainé“¾å¼æµå¼ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„chainè¿›è¡Œæµå¼è°ƒç”¨
            if self.chain:
                async for chunk in self.chain.astream({
                    "question": query,
                    "context": context
                }):
                    if chunk:
                        yield chunk
            else:
                raise Exception("LangChainå¤„ç†é“¾æœªåˆå§‹åŒ–")
                    
        except Exception as e:
            traceback.print_exc()
            raise Exception(f"LLMé“¾å¼æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
    
    async def _llm_answer(self, query: str, context: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Returns:
            str: LLMç”Ÿæˆçš„å›ç­”
        """
        try:
            # è°ƒç”¨LangChainå¤„ç†é“¾
            response = await self.chain.ainvoke({
                "question": query,
                "context": context
            })
            
            if not response or not response.strip():
                raise Exception("LLMè¿”å›ç©ºå›ç­”")
            
            return response.strip()
            
        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        ç®€å•å›ç­”æ¨¡å¼ - åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç®€å•æ¨¡å¼ç”Ÿæˆçš„å›ç­”
        """
        try:
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            
            # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
            best_chunk = context_chunks[0]
            document_name = best_chunk['document'].original_filename or best_chunk['document'].filename
            content = best_chunk['chunk'].content
            similarity = best_chunk['similarity']
            
            # æ„å»ºç®€å•å›ç­”ï¼ˆä¸åŒ…å«å‚è€ƒæ¥æºï¼‰
            answer = f"""åŸºäºæ–‡æ¡£ã€Š{document_name}ã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼š

{content}

ğŸ’¡ **æç¤º**ï¼šå½“å‰ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼ã€‚é…ç½®OpenAI API Keyåå¯è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"""
            
            return answer
            
        except Exception:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def _build_sources_info(self, context_chunks: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºæ¥æºä¿¡æ¯
        
        Args:
            context_chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ¥æºä¿¡æ¯
        """
        try:
            sources = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                similarity = chunk['similarity']
                sources.append(f"â€¢ {doc_name} (ç›¸ä¼¼åº¦: {similarity:.1%})")
            
            sources_text = "\n".join(sources)
            return f"""---
ğŸ“‹ **å‚è€ƒæ¥æº**ï¼š
{sources_text}

ğŸ’¡ åŸºäº {len(context_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µç”Ÿæˆæ­¤å›ç­”"""
            
        except Exception:
            return "---\nğŸ“‹ **å‚è€ƒæ¥æº**ï¼šä¿¡æ¯è·å–å¤±è´¥"


# å…¨å±€å®ä¾‹ - å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿æ•´ä¸ªåº”ç”¨ä½¿ç”¨ç›¸åŒçš„å®ä¾‹
try:
    # æ–‡æ¡£å¤„ç†å™¨å®ä¾‹
    document_processor = DocumentProcessor()
    
    # å‘é‡æœç´¢å®ä¾‹
    vector_search = VectorSearch()
    
    # RAGç”Ÿæˆå™¨å®ä¾‹
    rag_generator = RAGGenerator()
    
except Exception as e:
    raise Exception(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
