"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—

è¯¥æ¨¡å—åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
1. DocumentProcessor: æ–‡æ¡£å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–
2. VectorSearch: å‘é‡æœç´¢å¼•æ“ï¼ŒåŸºäºChromaæ•°æ®åº“è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
3. RAGGenerator: RAGç”Ÿæˆå™¨ï¼Œé›†æˆLangChainå’ŒOpenAIè¿›è¡Œæ™ºèƒ½é—®ç­”

æŠ€æœ¯æ ˆï¼š
- æ–‡æ¡£è§£æ: ç‹¬ç«‹çš„DocumentParseræ¨¡å—
- æ–‡æœ¬åˆ†å‰²: LangChain RecursiveCharacterTextSplitter
- å‘é‡åŒ–: Sentence Transformers
- å‘é‡å­˜å‚¨: ChromaDB
- æ™ºèƒ½é—®ç­”: LangChain + OpenAI GPT

æ¶æ„è¯´æ˜ï¼š
- æ–‡æ¡£è§£æåŠŸèƒ½å·²æ‹†åˆ†åˆ°ç‹¬ç«‹çš„document_parseræ¨¡å—
- æœ¬æ¨¡å—ä¸“æ³¨äºå‘é‡åŒ–ã€æœç´¢å’Œç”ŸæˆåŠŸèƒ½
- é€šè¿‡ä¾èµ–æ³¨å…¥çš„æ–¹å¼ä½¿ç”¨æ–‡æ¡£è§£æå™¨
- ç°åœ¨æ”¯æŒLangChainé›†æˆï¼Œæä¾›æ›´å¼ºå¤§çš„æ–‡æ¡£å¤„ç†èƒ½åŠ›
"""

import os
import traceback
from typing import List, Dict, Any

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

from config import (
    OPENAI_API_KEY, OPENAI_BASE_URL, CHROMA_PERSIST_DIRECTORY,
    CHROMA_COLLECTION, EMBEDDING_MODEL, HF_HOME, HF_OFFLINE,
    SIMILARITY_THRESHOLD, CHUNK_SIZE, CHUNK_OVERLAP
)
from apps.utils.common import get_local_model_path
from apps.models.document import Document as DocumentModel, DocumentChunk


class VectorStore:
    """
    å‘é‡æœç´¢å¼•æ“ - åŸºäºChromaDBçš„è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    
    åŠŸèƒ½ï¼š
    1. æ–‡æ¡£å‘é‡åŒ–å’Œå­˜å‚¨
    2. è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    3. æ–‡æ¡£åˆ é™¤
    4. è‡ªåŠ¨å‘é‡åŒ–å¤„ç†
    """

    def __init__(self):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        """
        try:
            # é…ç½®HuggingFaceç¯å¢ƒ
            os.environ["HF_HOME"] = HF_HOME
            os.environ["TRANSFORMERS_CACHE"] = HF_HOME
            os.environ["HF_HUB_CACHE"] = HF_HOME

            if HF_OFFLINE:
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"

            # å…³é—­Chromaé¥æµ‹
            os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
            os.environ.setdefault("CHROMA_TELEMETRY", "false")

            # åˆå§‹åŒ–å‘é‡åµŒå…¥æ¨¡å‹ï¼ˆä¸DocumentProcessorä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼‰
            local_model_path = get_local_model_path(EMBEDDING_MODEL, HF_HOME)

            if local_model_path and HF_OFFLINE:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=local_model_path,
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
            else:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=EMBEDDING_MODEL,
                    cache_folder=HF_HOME,
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )

            # åˆå§‹åŒ–ChromaDBå‘é‡å­˜å‚¨
            self.vectorstore = Chroma(
                collection_name=CHROMA_COLLECTION,
                embedding_function=self.embeddings,
                persist_directory=CHROMA_PERSIST_DIRECTORY,
            )

        except Exception as e:
            raise Exception(f"VectorSearchåˆå§‹åŒ–å¤±è´¥: {e}")

    async def add_documents_from_chunks(self, document_id: int, chunks: List[str], chunk_objects: List,
                                        metadata: Dict[str, Any] = None) -> List[str]:
        """
        ä»å·²åˆ†å—çš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        
        Args:
            document_id: æ–‡æ¡£ID
            chunks: å·²åˆ†å—çš„æ–‡æœ¬åˆ—è¡¨
            chunk_objects: å·²åˆ›å»ºçš„DocumentChunkå¯¹è±¡åˆ—è¡¨
            metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            List[str]: æ·»åŠ çš„æ–‡æ¡£å—IDåˆ—è¡¨
        """
        try:
            if not chunks or not chunk_objects:
                raise Exception("æ–‡æ¡£å—ä¸ºç©º")

            # åˆ›å»ºLangChainæ–‡æ¡£å¯¹è±¡
            documents = []
            chunk_ids = []

            for i, (chunk_text, chunk_obj) in enumerate(zip(chunks, chunk_objects)):
                # åˆ›å»ºLangChainæ–‡æ¡£å¯¹è±¡
                doc_metadata = {
                    "document_id": document_id,
                    "chunk_id": chunk_obj.id,
                    "chunk_index": i,
                    "source": metadata.get("filename",
                                           f"document_{document_id}") if metadata else f"document_{document_id}",
                }

                langchain_doc = Document(
                    page_content=chunk_text,
                    metadata=doc_metadata
                )

                documents.append(langchain_doc)
                chunk_ids.append(str(chunk_obj.id))

            # æ‰¹é‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            if documents:
                self.vectorstore.add_documents(documents)

            return chunk_ids

        except Exception as e:
            raise Exception(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨å¤±è´¥: {e}")

    async def search_similar_documents(self, query: str, top_k: int = 5, use_threshold: bool = True) -> List[
        Dict[str, Any]]:
        """
        æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            
        Returns:
            List[Dict]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨LangChainè¿›è¡Œç›¸ä¼¼åº¦æœç´¢ï¼Œè·å–æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤
            results = self.vectorstore.similarity_search_with_score(
                query, k=top_k  # è·å–æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤
            )

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            all_results = []
            filtered_results = []
            
            for doc, distance in results:
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç¡®ä¿ä¸ä¸ºè´Ÿ
                similarity = max(0.0, 1.0 - distance)

                # ä»metadataè·å–ä¿¡æ¯
                metadata = doc.metadata
                document_id = metadata.get('document_id')
                chunk_id = metadata.get('chunk_id')

                if document_id and chunk_id:
                    try:
                        # è·å–æ•°æ®åº“ä¸­çš„æ–‡æ¡£å’Œå—ä¿¡æ¯
                        document = await DocumentModel.get_or_none(id=document_id)
                        chunk = await DocumentChunk.get_or_none(id=chunk_id)

                        if document and chunk:
                            result_item = {
                                'document': document,
                                'chunk': chunk,
                                'similarity': similarity,
                                'metadata': metadata,
                                'above_threshold': similarity >= SIMILARITY_THRESHOLD
                            }
                            
                            all_results.append(result_item)
                            
                            # å¦‚æœä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåªä¿ç•™ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ç»“æœ
                            if use_threshold and similarity >= SIMILARITY_THRESHOLD:
                                filtered_results.append(result_item)
                    except Exception as e:
                        print(f"å¤„ç†æœç´¢ç»“æœé¡¹å¤±è´¥ (chunk_id: {metadata.get('chunk_id', 'unknown')}): {e}")
                        continue

            # é€‰æ‹©è¿”å›ç»“æœ
            if filtered_results:
                # æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œè¿”å›è¿‡æ»¤åçš„ç»“æœ
                filtered_results.sort(key=lambda x: x['similarity'], reverse=True)
                return filtered_results[:top_k]
            elif all_results:
                # æ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œè¿”å›æ‰€æœ‰ç»“æœï¼ˆé™çº§å¤„ç†ï¼‰
                all_results.sort(key=lambda x: x['similarity'], reverse=True)
                return all_results[:min(top_k, 3)]  # æœ€å¤šè¿”å›3ä¸ªç»“æœ
            else:
                return []

        except Exception as e:
            print(f"æœç´¢ç›¸ä¼¼æ–‡æ¡£å¤±è´¥: {e}")
            return []

    async def delete_document(self, document_id: int):
        """
        åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡æ•°æ®
        
        Args:
            document_id: æ–‡æ¡£ID
        """
        try:
            # ç›´æ¥ä½¿ç”¨ChromaDBå®¢æˆ·ç«¯åˆ é™¤ï¼Œè¿™æ˜¯æœ€å¯é çš„æ–¹æ³•
            # é€šè¿‡metadataæ¡ä»¶åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡
            self.vectorstore._collection.delete(where={"document_id": document_id})
            print(f"âœ… æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id} çš„å‘é‡æ•°æ®")

        except Exception as e:
            raise Exception(f"åˆ é™¤æ–‡æ¡£ {document_id} å‘é‡æ•°æ®å¤±è´¥: {e}")

    async def search_similar_chunks_with_mmr(self, query: str, top_k: int = 5,
                                              use_threshold: bool = True) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨MMRç®—æ³•æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£å—ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_threshold: æ˜¯å¦ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤

        Returns:
            List[Dict]: ç›¸ä¼¼æ–‡æ¡£å—åˆ—è¡¨
        """
        # ä½¿ç”¨æ”¹è¿›çš„æœç´¢é€»è¾‘ï¼Œç¡®ä¿æ€»æ˜¯æœ‰ç»“æœè¿”å›
        return await self.search_similar_documents(
            query=query,
            top_k=top_k,
            use_threshold=use_threshold
        )


class RAGGenerator:
    """
    RAGç”Ÿæˆå™¨ - æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. é›†æˆå‘é‡æœç´¢å’ŒLLMç”Ÿæˆ
    2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
    3. æ”¯æŒå¤šç§å›ç­”æ¨¡å¼ï¼ˆLLMæ™ºèƒ½å›ç­” / ç®€å•å›ç­”ï¼‰
    4. æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—®ç­”ä½“éªŒ
    """

    def __init__(self):
        """
        åˆå§‹åŒ–RAGç”Ÿæˆå™¨
        
        ç»„ä»¶ï¼š
        1. LangChainå‘é‡å­˜å‚¨: å‘é‡æœç´¢å¼•æ“
        2. LangChain + OpenAI: æ™ºèƒ½é—®ç­”é“¾ï¼ˆå¯é€‰ï¼‰
        3. å¤‡ç”¨ç®€å•å›ç­”æ¨¡å¼
        """
        try:
            # ä½¿ç”¨LangChainå‘é‡å­˜å‚¨
            self.vector_search = VectorStore()

            # åˆå§‹åŒ–LLMç»„ä»¶
            self.llm = None
            self.chain = None
            self.llm_available = False

            # æ£€æŸ¥OpenAIé…ç½®å¹¶åˆå§‹åŒ–LLM
            if OPENAI_API_KEY and OPENAI_API_KEY.strip():
                try:
                    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
                    self.llm = ChatOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿å›ç­”çš„ä¸€è‡´æ€§
                        model="gpt-3.5-turbo",
                        max_tokens=2000  # é™åˆ¶å›ç­”é•¿åº¦
                    )

                    # åˆ›å»ºç³»ç»Ÿæç¤ºæ¨¡æ¿
                    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„ï¼š
   - ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜ç»„ç»‡å†…å®¹
   - é‡è¦ä¿¡æ¯ç”¨**ç²—ä½“**æ ‡è®°
   - ä½¿ç”¨é¡¹ç›®ç¬¦å·(â€¢)æˆ–æ•°å­—åˆ—è¡¨å±•ç¤ºè¦ç‚¹
   - æ¯ä¸ªæ®µè½ä¸“æ³¨ä¸€ä¸ªä¸»é¢˜ï¼Œæ®µè½é—´ç•™ç©ºè¡Œ
   - å¤æ‚å†…å®¹ä½¿ç”¨è¡¨æ ¼æˆ–ç»“æ„åŒ–æ ¼å¼
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œå…³é”®å†…å®¹ç‰‡æ®µï¼Œæ ¼å¼ï¼šã€Œæ–‡æ¡£åï¼šå…·ä½“å†…å®¹ã€
5. å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£æä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶æ ‡æ˜ä¿¡æ¯æ¥æº
6. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚å½“ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}"""

                    human_template = "é—®é¢˜ï¼š{question}"

                    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
                    chat_prompt = ChatPromptTemplate([
                        ("system", system_template),
                        ("human", human_template),
                    ])

                    # åˆ›å»ºè¾“å‡ºè§£æå™¨
                    output_parser = StrOutputParser()

                    # åˆ›å»ºLangChainå¤„ç†é“¾
                    self.chain = chat_prompt | self.llm | output_parser
                    self.llm_available = True

                except Exception:
                    self.llm = None
                    self.chain = None
                    self.llm_available = False
            else:
                self.llm_available = False

        except Exception as e:
            raise Exception(f"RAGGeneratoråˆå§‹åŒ–å¤±è´¥: {e}")

    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        å›ç­”ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨LLMæ™ºèƒ½å›ç­”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. é™çº§åˆ°ç®€å•å›ç­”æ¨¡å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        3. æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ¥æºå¼•ç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"

            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                # å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼
                if chunk.get('document') and chunk.get('chunk'):
                    doc_name = chunk['document'].original_filename or chunk['document'].filename
                    content = chunk['chunk'].content
                else:
                    # ä½¿ç”¨å¤‡ç”¨æ ¼å¼
                    doc_name = chunk.get('metadata', {}).get('source', 'æœªçŸ¥æ–‡æ¡£')
                    content = chunk.get('content', 'æ— å†…å®¹')
                
                similarity = chunk['similarity']

                # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
                context_part = f"""æ–‡æ¡£ {i}: {doc_name}
ç›¸ä¼¼åº¦: {similarity:.2f}
å†…å®¹: {content}"""
                context_parts.append(context_part)

            context = "\n\n" + "=" * 50 + "\n\n".join(context_parts)

            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    answer = await self._llm_answer(query, context)

                    # ä¸å†è‡ªåŠ¨æ·»åŠ æ¥æºä¿¡æ¯ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦æ˜¾ç¤º
                    return answer

                except Exception:
                    return self._simple_answer(query, context_chunks)
            else:
                return self._simple_answer(query, context_chunks)

        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def generate_answer_stream(self, query: str, context_chunks: List[Dict[str, Any]]):
        """
        æµå¼ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ–‡æ¡£ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                # ç›´æ¥ä½¿ç”¨æ•°æ®ç»“æ„
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                content = chunk['chunk'].content
                context_parts.append(f"æ–‡æ¡£{i}: {doc_name}\nå†…å®¹: {content}")

            context = "\n\n" + "=" * 50 + "\n\n".join(context_parts)

            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    async for chunk in self._llm_answer_stream(query, context):
                        yield chunk
                except Exception as e:
                    print(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {e}")
                    # é™çº§åˆ°ç®€å•å›ç­”
                    simple_answer = self._simple_answer(query, context_chunks)
                    yield simple_answer
            else:
                # éLLMæ¨¡å¼ï¼Œç›´æ¥è¿”å›ç®€å•å›ç­”
                simple_answer = self._simple_answer(query, context_chunks)
                yield simple_answer

        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def _llm_answer_stream(self, query: str, context: str):
        """
        ä½¿ç”¨LangChainé“¾å¼æµå¼ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„chainè¿›è¡Œæµå¼è°ƒç”¨
            if self.chain:
                async for chunk in self.chain.astream({
                    "question": query,
                    "context": context
                }):
                    if chunk:
                        yield chunk
            else:
                raise Exception("LangChainå¤„ç†é“¾æœªåˆå§‹åŒ–")

        except Exception as e:
            traceback.print_exc()
            raise Exception(f"LLMé“¾å¼æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")

    async def _llm_answer(self, query: str, context: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Returns:
            str: LLMç”Ÿæˆçš„å›ç­”
        """
        try:
            # è°ƒç”¨LangChainå¤„ç†é“¾
            response = await self.chain.ainvoke({
                "question": query,
                "context": context
            })

            if not response or not response.strip():
                raise Exception("LLMè¿”å›ç©ºå›ç­”")

            return response.strip()

        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        ç®€å•å›ç­”æ¨¡å¼ - åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç®€å•æ¨¡å¼ç”Ÿæˆçš„å›ç­”
        """
        try:
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

            # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
            best_chunk = context_chunks[0]
            document_name = best_chunk['document'].original_filename or best_chunk['document'].filename
            content = best_chunk['chunk'].content
            similarity = best_chunk['similarity']

            # æ„å»ºç®€å•å›ç­”ï¼ˆä¸åŒ…å«å‚è€ƒæ¥æºï¼‰
            answer = f"""åŸºäºæ–‡æ¡£ã€Š{document_name}ã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼š

{content}

ğŸ’¡ **æç¤º**ï¼šå½“å‰ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼ã€‚é…ç½®OpenAI API Keyåå¯è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"""

            return answer

        except Exception:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"

    def _build_sources_info(self, context_chunks: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºæ¥æºä¿¡æ¯
        
        Args:
            context_chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ¥æºä¿¡æ¯
        """
        try:
            sources = []
            for i, chunk in enumerate(context_chunks, 1):
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                similarity = chunk['similarity']
                sources.append(f"â€¢ {doc_name} (ç›¸ä¼¼åº¦: {similarity:.1%})")

            sources_text = "\n".join(sources)
            return f"""---
ğŸ“‹ **å‚è€ƒæ¥æº**ï¼š
{sources_text}

ğŸ’¡ åŸºäº {len(context_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µç”Ÿæˆæ­¤å›ç­”"""

        except Exception:
            return "---\nğŸ“‹ **å‚è€ƒæ¥æº**ï¼šä¿¡æ¯è·å–å¤±è´¥"


# å…¨å±€å®ä¾‹ - å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿æ•´ä¸ªåº”ç”¨ä½¿ç”¨ç›¸åŒçš„å®ä¾‹
try:
    # ä½¿ç”¨LangChainå‘é‡å­˜å‚¨
    vector_search = VectorStore()
    print("âœ… ä½¿ç”¨LangChainå‘é‡å­˜å‚¨")

    # RAGç”Ÿæˆå™¨å®ä¾‹
    rag_generator = RAGGenerator()

except Exception as e:
    raise Exception(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
