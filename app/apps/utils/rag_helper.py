"""
RAG (Retrieval-Augmented Generation) ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—

è¯¥æ¨¡å—åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
1. DocumentProcessor: æ–‡æ¡£å¤„ç†å™¨ï¼Œè´Ÿè´£æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–
2. VectorSearch: å‘é‡æœç´¢å¼•æ“ï¼ŒåŸºäºChromaæ•°æ®åº“è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
3. RAGGenerator: RAGç”Ÿæˆå™¨ï¼Œé›†æˆLangChainå’ŒOpenAIè¿›è¡Œæ™ºèƒ½é—®ç­”

æŠ€æœ¯æ ˆï¼š
- æ–‡æ¡£è§£æ: ç‹¬ç«‹çš„DocumentParseræ¨¡å—
- æ–‡æœ¬åˆ†å‰²: LangChain RecursiveCharacterTextSplitter
- å‘é‡åŒ–: Sentence Transformers
- å‘é‡å­˜å‚¨: ChromaDB
- æ™ºèƒ½é—®ç­”: LangChain + OpenAI GPT

æ¶æ„è¯´æ˜ï¼š
- æ–‡æ¡£è§£æåŠŸèƒ½å·²æ‹†åˆ†åˆ°ç‹¬ç«‹çš„document_parseræ¨¡å—
- æœ¬æ¨¡å—ä¸“æ³¨äºå‘é‡åŒ–ã€æœç´¢å’Œç”ŸæˆåŠŸèƒ½
- é€šè¿‡ä¾èµ–æ³¨å…¥çš„æ–¹å¼ä½¿ç”¨æ–‡æ¡£è§£æå™¨
- ç°åœ¨æ”¯æŒLangChainé›†æˆï¼Œæä¾›æ›´å¼ºå¤§çš„æ–‡æ¡£å¤„ç†èƒ½åŠ›
"""

import traceback
from typing import List, Dict, Any

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from apps.utils.vector_db_selector import vector_search
from config import (
    OPENAI_API_KEY, OPENAI_BASE_URL
)


class RAGGenerator:
    """
    RAGç”Ÿæˆå™¨ - æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. é›†æˆå‘é‡æœç´¢å’ŒLLMç”Ÿæˆ
    2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
    3. æ”¯æŒå¤šç§å›ç­”æ¨¡å¼ï¼ˆLLMæ™ºèƒ½å›ç­” / ç®€å•å›ç­”ï¼‰
    4. æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—®ç­”ä½“éªŒ
    """

    def __init__(self):
        """
        åˆå§‹åŒ–RAGç”Ÿæˆå™¨
        
        ç»„ä»¶ï¼š
        1. LangChainå‘é‡å­˜å‚¨: å‘é‡æœç´¢å¼•æ“
        2. LangChain + OpenAI: æ™ºèƒ½é—®ç­”é“¾ï¼ˆå¯é€‰ï¼‰
        3. å¤‡ç”¨ç®€å•å›ç­”æ¨¡å¼
        """
        try:
            # ä½¿ç”¨LangChainå‘é‡å­˜å‚¨
            self.vector_search = vector_search

            # åˆå§‹åŒ–LLMç»„ä»¶
            self.llm = None
            self.chain = None
            self.llm_available = False

            # æ£€æŸ¥OpenAIé…ç½®å¹¶åˆå§‹åŒ–LLM
            if OPENAI_API_KEY and OPENAI_API_KEY.strip():
                try:
                    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
                    self.llm = ChatOpenAI(
                        api_key=OPENAI_API_KEY,
                        base_url=OPENAI_BASE_URL,
                        temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿å›ç­”çš„ä¸€è‡´æ€§
                        model="gpt-3.5-turbo",
                        max_tokens=2000  # é™åˆ¶å›ç­”é•¿åº¦
                    )

                    # åˆ›å»ºç³»ç»Ÿæç¤ºæ¨¡æ¿
                    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ä¸”æœ‰æ¡ç†ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½ç»“æ„ï¼š
   - ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜ç»„ç»‡å†…å®¹
   - é‡è¦ä¿¡æ¯ç”¨**ç²—ä½“**æ ‡è®°
   - ä½¿ç”¨é¡¹ç›®ç¬¦å·(â€¢)æˆ–æ•°å­—åˆ—è¡¨å±•ç¤ºè¦ç‚¹
   - æ¯ä¸ªæ®µè½ä¸“æ³¨ä¸€ä¸ªä¸»é¢˜ï¼Œæ®µè½é—´ç•™ç©ºè¡Œ
   - å¤æ‚å†…å®¹ä½¿ç”¨è¡¨æ ¼æˆ–ç»“æ„åŒ–æ ¼å¼
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œå…³é”®å†…å®¹ç‰‡æ®µï¼Œæ ¼å¼ï¼šã€Œæ–‡æ¡£åï¼šå…·ä½“å†…å®¹ã€
5. å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£æä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆåˆ†æå¹¶æ ‡æ˜ä¿¡æ¯æ¥æº
6. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚å½“ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}"""

                    human_template = "é—®é¢˜ï¼š{question}"

                    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
                    chat_prompt = ChatPromptTemplate([
                        ("system", system_template),
                        ("human", human_template),
                    ])

                    # åˆ›å»ºè¾“å‡ºè§£æå™¨
                    output_parser = StrOutputParser()

                    # åˆ›å»ºLangChainå¤„ç†é“¾
                    self.chain = chat_prompt | self.llm | output_parser
                    self.llm_available = True

                except Exception:
                    self.llm = None
                    self.chain = None
                    self.llm_available = False
            else:
                self.llm_available = False

        except Exception as e:
            raise Exception(f"RAGGeneratoråˆå§‹åŒ–å¤±è´¥: {e}")

    async def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        å›ç­”ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨LLMæ™ºèƒ½å›ç­”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. é™çº§åˆ°ç®€å•å›ç­”æ¨¡å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        3. æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ¥æºå¼•ç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"

            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                # å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼
                if chunk.get('document') and chunk.get('chunk'):
                    doc_name = chunk['document'].original_filename or chunk['document'].filename
                    content = chunk['chunk'].content
                else:
                    # ä½¿ç”¨å¤‡ç”¨æ ¼å¼
                    doc_name = chunk.get('metadata', {}).get('source', 'æœªçŸ¥æ–‡æ¡£')
                    content = chunk.get('content', 'æ— å†…å®¹')

                similarity = chunk['similarity']

                # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
                context_part = f"""æ–‡æ¡£ {i}: {doc_name}
ç›¸ä¼¼åº¦: {similarity:.2f}
å†…å®¹: {content}"""
                context_parts.append(context_part)

            context = "\n\n" + "=" * 50 + "\n\n".join(context_parts)

            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    answer = await self._llm_answer(query, context)

                    # ä¸å†è‡ªåŠ¨æ·»åŠ æ¥æºä¿¡æ¯ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦æ˜¾ç¤º
                    return answer

                except Exception:
                    return self._simple_answer(query, context_chunks)
            else:
                return self._simple_answer(query, context_chunks)

        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def generate_answer_stream(self, query: str, context_chunks: List[Dict[str, Any]]):
        """
        æµå¼ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ–‡æ¡£ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, chunk in enumerate(context_chunks, 1):
                # ç›´æ¥ä½¿ç”¨æ•°æ®ç»“æ„
                doc_name = chunk['document'].original_filename or chunk['document'].filename
                content = chunk['chunk'].content
                context_parts.append(f"æ–‡æ¡£{i}: {doc_name}\nå†…å®¹: {content}")

            context = "\n\n" + "=" * 50 + "\n\n".join(context_parts)

            # é€‰æ‹©å›ç­”æ¨¡å¼
            if self.llm_available and self.chain:
                try:
                    async for chunk in self._llm_answer_stream(query, context):
                        yield chunk
                except Exception as e:
                    print(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {e}")
                    # é™çº§åˆ°ç®€å•å›ç­”
                    simple_answer = self._simple_answer(query, context_chunks)
                    yield simple_answer
            else:
                # éLLMæ¨¡å¼ï¼Œç›´æ¥è¿”å›ç®€å•å›ç­”
                simple_answer = self._simple_answer(query, context_chunks)
                yield simple_answer

        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    async def _llm_answer_stream(self, query: str, context: str):
        """
        ä½¿ç”¨LangChainé“¾å¼æµå¼ç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            # ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„chainè¿›è¡Œæµå¼è°ƒç”¨
            if self.chain:
                async for chunk in self.chain.astream({
                    "question": query,
                    "context": context
                }):
                    if chunk:
                        yield chunk
            else:
                raise Exception("LangChainå¤„ç†é“¾æœªåˆå§‹åŒ–")

        except Exception as e:
            traceback.print_exc()
            raise Exception(f"LLMé“¾å¼æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")

    async def _llm_answer(self, query: str, context: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ–‡æ¡£ä¸Šä¸‹æ–‡
            
        Returns:
            str: LLMç”Ÿæˆçš„å›ç­”
        """
        try:
            # è°ƒç”¨LangChainå¤„ç†é“¾
            response = await self.chain.ainvoke({
                "question": query,
                "context": context
            })

            if not response or not response.strip():
                raise Exception("LLMè¿”å›ç©ºå›ç­”")

            return response.strip()

        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def _simple_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        ç®€å•å›ç­”æ¨¡å¼ - åŸºäºå…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æ¡£å—
            
        Returns:
            str: ç®€å•æ¨¡å¼ç”Ÿæˆçš„å›ç­”
        """
        try:
            if not context_chunks:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

            # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
            best_chunk = context_chunks[0]
            document_name = best_chunk['document'].original_filename or best_chunk['document'].filename
            content = best_chunk['chunk'].content

            # æ„å»ºç®€å•å›ç­”ï¼ˆä¸åŒ…å«å‚è€ƒæ¥æºï¼‰
            answer = f"""åŸºäºæ–‡æ¡£ã€Š{document_name}ã€‹ä¸­çš„ç›¸å…³å†…å®¹ï¼š

{content}

ğŸ’¡ **æç¤º**ï¼šå½“å‰ä½¿ç”¨ç®€å•å›ç­”æ¨¡å¼ã€‚é…ç½®OpenAI API Keyåå¯è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"""

            return answer

        except Exception:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"


rag_generator = RAGGenerator()
